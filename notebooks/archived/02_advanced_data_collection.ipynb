{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# üöÄ Advanced Hong Kong Stock Data Collection\n",
    "\n",
    "**Perfect for:** Intermediate/Advanced users, large-scale analysis (100-500+ stocks)\n",
    "\n",
    "This notebook provides **advanced techniques** for efficient, large-scale Hong Kong stock data collection.\n",
    "\n",
    "## ‚ö° Advanced Features\n",
    "- Parallel processing with configurable workers\n",
    "- Enterprise-grade rate limiting and retry logic\n",
    "- Checkpoint/resume capabilities for large operations\n",
    "- Memory-efficient streaming for 500+ stocks\n",
    "- Advanced error recovery and fault tolerance\n",
    "- Performance monitoring and optimization\n",
    "\n",
    "## ‚è±Ô∏è Time Estimates\n",
    "- **100 stocks**: 15-25 minutes\n",
    "- **200 stocks**: 30-45 minutes  \n",
    "- **500 stocks**: 1-2 hours\n",
    "- **1000+ stocks**: 3-4 hours (enterprise mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced setup with performance monitoring\n",
    "from common_setup import setup_notebook, get_date_range, import_common_modules\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import concurrent.futures\n",
    "import time\n",
    "import psutil\n",
    "import gc\n",
    "\n",
    "# Initialize with performance tracking\n",
    "print(\"üöÄ Advanced Data Collection Setup\")\n",
    "print(f\"üíª System Memory: {psutil.virtual_memory().total / (1024**3):.1f} GB\")\n",
    "print(f\"üñ•Ô∏è CPU Cores: {psutil.cpu_count()}\")\n",
    "\n",
    "validation = setup_notebook()\n",
    "\n",
    "# Import advanced collection modules\n",
    "modules = import_common_modules()\n",
    "get_hk_stock_list_static = modules['get_hk_stock_list_static']\n",
    "\n",
    "# Import enterprise-grade functions\n",
    "from bulk_collection_improved import (\n",
    "    BulkCollectionConfig,\n",
    "    BulkCollector,\n",
    "    ResultsManager,\n",
    "    create_enterprise_collector\n",
    ")\n",
    "\n",
    "from hk_stock_universe import (\n",
    "    get_comprehensive_hk_stock_list,\n",
    "    get_hk_stocks_by_sector,\n",
    "    MAJOR_HK_STOCKS\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Advanced setup completed - Ready for large-scale collection!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Advanced Configuration Profiles\n",
    "\n",
    "Choose from pre-configured profiles optimized for different scales:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define advanced configuration profiles\n",
    "PROFILES = {\n",
    "    'intermediate': {\n",
    "        'max_stocks': 100,\n",
    "        'batch_size': 10,\n",
    "        'max_workers': 2,\n",
    "        'delay_between_batches': 1.0,\n",
    "        'enable_parallel': True,\n",
    "        'checkpoint_every': 25,\n",
    "        'memory_limit_gb': 4\n",
    "    },\n",
    "    \n",
    "    'advanced': {\n",
    "        'max_stocks': 300,\n",
    "        'batch_size': 15,\n",
    "        'max_workers': 3,\n",
    "        'delay_between_batches': 0.8,\n",
    "        'enable_parallel': True,\n",
    "        'checkpoint_every': 50,\n",
    "        'memory_limit_gb': 8\n",
    "    },\n",
    "    \n",
    "    'enterprise': {\n",
    "        'max_stocks': 1000,\n",
    "        'batch_size': 20,\n",
    "        'max_workers': 4,\n",
    "        'delay_between_batches': 0.5,\n",
    "        'enable_parallel': True,\n",
    "        'checkpoint_every': 100,\n",
    "        'memory_limit_gb': 16\n",
    "    }\n",
    "}\n",
    "\n",
    "# Select your profile (modify as needed)\n",
    "SELECTED_PROFILE = 'intermediate'  # Options: intermediate, advanced, enterprise\n",
    "\n",
    "config = PROFILES[SELECTED_PROFILE].copy()\n",
    "config.update({\n",
    "    'date_range_days': 365,\n",
    "    'force_refresh': False,\n",
    "    'enable_resume': True,\n",
    "    'save_checkpoints': True\n",
    "})\n",
    "\n",
    "print(f\"‚öôÔ∏è **Selected Profile: {SELECTED_PROFILE.upper()}**\")\n",
    "print(\"=\" * 50)\n",
    "for key, value in config.items():\n",
    "    print(f\"   {key}: {value}\")\n",
    "\n",
    "# Calculate date range\n",
    "start_date, end_date = get_date_range(config['date_range_days'])\n",
    "print(f\"\\nüìÖ **Date Range:** {start_date} to {end_date}\")\n",
    "\n",
    "# Memory check\n",
    "current_memory = psutil.virtual_memory()\n",
    "if current_memory.available / (1024**3) < config['memory_limit_gb']:\n",
    "    print(f\"‚ö†Ô∏è **Memory Warning:** Available memory ({current_memory.available / (1024**3):.1f} GB) < recommended ({config['memory_limit_gb']} GB)\")\n",
    "    print(\"Consider reducing max_stocks or using a lower profile\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## üéØ Advanced Stock Universe Discovery\n",
    "\n",
    "Discover and categorize the complete Hong Kong stock universe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced stock universe discovery\n",
    "print(\"üîç **Hong Kong Stock Universe Discovery**\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Get comprehensive stock list\n",
    "try:\n",
    "    comprehensive_stocks = get_comprehensive_hk_stock_list()\n",
    "    print(f\"üìà **Comprehensive Universe:** {len(comprehensive_stocks)} stocks\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Using fallback static list: {e}\")\n",
    "    comprehensive_stocks = get_hk_stock_list_static()\n",
    "\n",
    "# Categorize by sectors\n",
    "sector_analysis = {}\n",
    "total_sector_stocks = 0\n",
    "\n",
    "for sector, stocks in MAJOR_HK_STOCKS.items():\n",
    "    sector_stocks = get_hk_stocks_by_sector(sector)\n",
    "    sector_analysis[sector] = {\n",
    "        'count': len(sector_stocks),\n",
    "        'stocks': sector_stocks,\n",
    "        'percentage': len(sector_stocks) / len(comprehensive_stocks) * 100\n",
    "    }\n",
    "    total_sector_stocks += len(sector_stocks)\n",
    "\n",
    "print(f\"\\nüìä **Sector Analysis:**\")\n",
    "for sector, data in sector_analysis.items():\n",
    "    print(f\"   üè¢ {sector.upper()}: {data['count']} stocks ({data['percentage']:.1f}%)\")\n",
    "\n",
    "print(f\"\\nüìã **Universe Statistics:**\")\n",
    "print(f\"   Total discovered: {len(comprehensive_stocks)}\")\n",
    "print(f\"   Categorized in sectors: {total_sector_stocks}\")\n",
    "print(f\"   Uncategorized: {len(comprehensive_stocks) - total_sector_stocks}\")\n",
    "\n",
    "# Select target universe based on profile\n",
    "if config['max_stocks'] >= len(comprehensive_stocks):\n",
    "    target_universe = comprehensive_stocks\n",
    "    print(f\"\\nüéØ **Target: Full Universe** ({len(target_universe)} stocks)\")\n",
    "else:\n",
    "    # Prioritize major stocks and sector diversity\n",
    "    priority_stocks = []\n",
    "    \n",
    "    # Add major stocks first\n",
    "    major_stocks = get_hk_stock_list_static()\n",
    "    priority_stocks.extend(major_stocks[:config['max_stocks']//2])\n",
    "    \n",
    "    # Add sector diversity\n",
    "    remaining_slots = config['max_stocks'] - len(priority_stocks)\n",
    "    stocks_per_sector = remaining_slots // len(sector_analysis)\n",
    "    \n",
    "    for sector, data in sector_analysis.items():\n",
    "        sector_stocks = [s for s in data['stocks'] if s not in priority_stocks]\n",
    "        priority_stocks.extend(sector_stocks[:stocks_per_sector])\n",
    "    \n",
    "    target_universe = priority_stocks[:config['max_stocks']]\n",
    "    print(f\"\\nüéØ **Target: Curated Selection** ({len(target_universe)} stocks)\")\n",
    "\n",
    "print(f\"üìã **Sample targets:** {', '.join(target_universe[:10])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## üîÑ Advanced Collection with Parallel Processing\n",
    "\n",
    "Execute large-scale collection with enterprise features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize advanced collector\n",
    "print(\"üöÄ **Initializing Advanced Collection Engine**\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create enterprise collector with selected configuration\n",
    "collector_config = BulkCollectionConfig()\n",
    "collector_config.max_workers = config['max_workers']\n",
    "collector_config.batch_size = config['batch_size']\n",
    "collector_config.delay_between_batches = config['delay_between_batches']\n",
    "collector_config.enable_parallel = config['enable_parallel']\n",
    "\n",
    "# Initialize collector\n",
    "advanced_collector = create_enterprise_collector(collector_config)\n",
    "\n",
    "# Performance monitoring setup\n",
    "start_time = time.time()\n",
    "initial_memory = psutil.virtual_memory().used / (1024**3)\n",
    "\n",
    "print(f\"‚öôÔ∏è **Collection Configuration:**\")\n",
    "print(f\"   Parallel processing: {'‚úÖ Enabled' if config['enable_parallel'] else '‚ùå Disabled'}\")\n",
    "print(f\"   Workers: {config['max_workers']}\")\n",
    "print(f\"   Batch size: {config['batch_size']}\")\n",
    "print(f\"   Checkpoint every: {config['checkpoint_every']} stocks\")\n",
    "print(f\"   Estimated time: {len(target_universe) * config['delay_between_batches'] / (config['max_workers'] * 60):.1f} minutes\")\n",
    "\n",
    "# Memory monitoring function\n",
    "def monitor_memory():\n",
    "    current = psutil.virtual_memory()\n",
    "    used_gb = current.used / (1024**3)\n",
    "    available_gb = current.available / (1024**3)\n",
    "    print(f\"üíæ Memory: {used_gb:.1f}GB used, {available_gb:.1f}GB available ({current.percent:.1f}%)\")\n",
    "\n",
    "monitor_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## üéØ Execute Advanced Collection\n",
    "\n",
    "Run the large-scale collection with checkpointing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute advanced collection with checkpointing\n",
    "print(\"üöÄ **Starting Advanced Bulk Collection**\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "collection_results = {}\n",
    "checkpoint_data = {}\n",
    "failed_stocks = []\n",
    "\n",
    "try:\n",
    "    # Configure logging verbosity\n",
    "    VERBOSE_LOGGING = False  # Set to True for detailed batch logging\n",
    "    SHOW_MEMORY = True      # Set to False to hide memory monitoring\n",
    "    \n",
    "    # Process in checkpointed batches with smart progress tracking\n",
    "    total_batches = (len(target_universe) + config['checkpoint_every'] - 1) // config['checkpoint_every']\n",
    "    \n",
    "    print(f\"\\nüîÑ Processing {len(target_universe)} stocks in {total_batches} checkpointed batches...\")\n",
    "    \n",
    "    with tqdm(total=len(target_universe), desc=\"Enterprise Collection\", unit=\"stocks\") as pbar:\n",
    "        for i in range(0, len(target_universe), config['checkpoint_every']):\n",
    "            batch_end = min(i + config['checkpoint_every'], len(target_universe))\n",
    "            current_batch = target_universe[i:batch_end]\n",
    "            batch_num = i//config['checkpoint_every'] + 1\n",
    "            \n",
    "            if VERBOSE_LOGGING:\n",
    "                print(f\"\\nüì¶ **Processing Batch {batch_num}** ({len(current_batch)} stocks)\")\n",
    "                print(f\"   Range: {i+1} to {batch_end} of {len(target_universe)}\")\n",
    "            \n",
    "            # Monitor memory before batch (only if enabled)\n",
    "            if SHOW_MEMORY and VERBOSE_LOGGING:\n",
    "                monitor_memory()\n",
    "            \n",
    "            # Process current batch\n",
    "            try:\n",
    "                batch_results = advanced_collector.fetch_stocks_parallel(\n",
    "                    tickers=current_batch,\n",
    "                    start_date=start_date,\n",
    "                    end_date=end_date,\n",
    "                    verbose=False  # Suppress per-stock logs\n",
    "                )\n",
    "                \n",
    "                # Merge results\n",
    "                collection_results.update(batch_results)\n",
    "                \n",
    "                # Calculate batch statistics\n",
    "                batch_success = len(batch_results)\n",
    "                batch_failed = len(current_batch) - batch_success\n",
    "                success_rate = batch_success / len(current_batch) * 100\n",
    "                \n",
    "                if VERBOSE_LOGGING:\n",
    "                    print(f\"   ‚úÖ Batch completed: {batch_success}/{len(current_batch)} ({success_rate:.1f}% success)\")\n",
    "                \n",
    "                # Track failed stocks (quietly unless verbose)\n",
    "                if batch_failed > 0:\n",
    "                    failed_batch = [stock for stock in current_batch if stock not in batch_results]\n",
    "                    failed_stocks.extend(failed_batch)\n",
    "                    if VERBOSE_LOGGING and batch_failed <= 5:\n",
    "                        print(f\"   ‚ùå Failed stocks: {', '.join(failed_batch)}\")\n",
    "                    elif VERBOSE_LOGGING:\n",
    "                        print(f\"   ‚ùå Failed: {batch_failed} stocks\")\n",
    "                \n",
    "                # Save checkpoint (quietly)\n",
    "                if config['save_checkpoints']:\n",
    "                    checkpoint_data = {\n",
    "                        'completed_stocks': list(collection_results.keys()),\n",
    "                        'failed_stocks': failed_stocks,\n",
    "                        'batch_number': batch_num,\n",
    "                        'timestamp': datetime.now().isoformat()\n",
    "                    }\n",
    "                    \n",
    "                # Force garbage collection after each batch\n",
    "                gc.collect()\n",
    "                \n",
    "                # Update progress bar with comprehensive info\n",
    "                total_completed = len(collection_results)\n",
    "                overall_progress = total_completed / len(target_universe) * 100\n",
    "                elapsed_time = time.time() - start_time\n",
    "                \n",
    "                # Calculate ETA\n",
    "                if total_completed > 0:\n",
    "                    eta_minutes = (elapsed_time / total_completed) * (len(target_universe) - total_completed) / 60\n",
    "                    pbar.set_postfix({\n",
    "                        'Batch': f\"{batch_num}/{total_batches}\",\n",
    "                        'Success': f\"{success_rate:.0f}%\",\n",
    "                        'Overall': f\"{overall_progress:.1f}%\",\n",
    "                        'ETA': f\"{eta_minutes:.0f}m\"\n",
    "                    })\n",
    "                \n",
    "                pbar.update(len(current_batch))\n",
    "                \n",
    "                # Show progress summary every 5 batches (if verbose)\n",
    "                if VERBOSE_LOGGING and batch_num % 5 == 0:\n",
    "                    print(f\"   üìä Overall progress: {total_completed}/{len(target_universe)} ({overall_progress:.1f}%)\")\n",
    "                    print(f\"   ‚è±Ô∏è Elapsed: {elapsed_time/60:.1f}m, ETA: {eta_minutes:.1f}m\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                if VERBOSE_LOGGING:\n",
    "                    print(f\"   ‚ùå Batch error: {e}\")\n",
    "                failed_stocks.extend(current_batch)\n",
    "                pbar.update(len(current_batch))\n",
    "                continue\n",
    "    \n",
    "    # Print comprehensive collection summary\n",
    "    from common_setup import print_collection_summary\n",
    "    print_collection_summary(\n",
    "        collected_data=collection_results,\n",
    "        failed_stocks=failed_stocks,\n",
    "        target_count=len(target_universe),\n",
    "        start_time=start_time,\n",
    "        show_failed_details=True,\n",
    "        max_failed_shown=15  # Show more details for advanced users\n",
    "    )\n",
    "    \n",
    "except KeyboardInterrupt:\n",
    "    print(f\"\\n‚è∏Ô∏è **Collection Interrupted**\")\n",
    "    print(f\"   Partial results available: {len(collection_results)} stocks\")\n",
    "    \n",
    "    # Show interrupted collection summary\n",
    "    from common_setup import print_collection_summary\n",
    "    print_collection_summary(\n",
    "        collected_data=collection_results,\n",
    "        failed_stocks=failed_stocks,\n",
    "        target_count=len(target_universe),\n",
    "        start_time=start_time,\n",
    "        show_failed_details=True,\n",
    "        max_failed_shown=5\n",
    "    )\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå **Collection Error:** {e}\")\n",
    "\n",
    "# Additional performance metrics for advanced users\n",
    "total_time = time.time() - start_time\n",
    "final_memory = psutil.virtual_memory().used / (1024**3)\n",
    "memory_used = final_memory - initial_memory\n",
    "\n",
    "print(f\"\\nüíª **Advanced Performance Metrics:**\")\n",
    "print(f\"   üíæ Memory used: {memory_used:.1f} GB\")\n",
    "print(f\"   üß† Memory per stock: {memory_used/len(collection_results):.3f} GB\" if len(collection_results) > 0 else \"   üß† Memory per stock: N/A\")\n",
    "print(f\"   ‚ö° Peak memory efficiency: {len(collection_results)/memory_used:.1f} stocks/GB\" if memory_used > 0 else \"   ‚ö° Peak memory efficiency: N/A\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## üìä Advanced Data Quality Analysis\n",
    "\n",
    "Comprehensive analysis of collected data quality and performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced quality analysis\n",
    "if collection_results:\n",
    "    print(\"üìä **Advanced Data Quality Analysis**\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Create comprehensive summary\n",
    "    summary_data = []\n",
    "    total_records = 0\n",
    "    \n",
    "    for ticker, data in collection_results.items():\n",
    "        if data is not None and not data.empty:\n",
    "            record_count = len(data)\n",
    "            total_records += record_count\n",
    "            \n",
    "            # Calculate quality metrics\n",
    "            completeness = record_count / 252 * 100  # Assume 252 trading days per year\n",
    "            price_range = data['Close'].max() - data['Close'].min()\n",
    "            volatility = data['Close'].std()\n",
    "            avg_volume = data['Volume'].mean()\n",
    "            \n",
    "            summary_data.append({\n",
    "                'Ticker': ticker,\n",
    "                'Records': record_count,\n",
    "                'Completeness': f\"{completeness:.1f}%\",\n",
    "                'Price_Range': f\"${price_range:.2f}\",\n",
    "                'Volatility': f\"{volatility:.2f}\",\n",
    "                'Avg_Volume': f\"{avg_volume:,.0f}\",\n",
    "                'Start_Date': data.index[0].date(),\n",
    "                'End_Date': data.index[-1].date()\n",
    "            })\n",
    "    \n",
    "    # Create analysis DataFrame\n",
    "    analysis_df = pd.DataFrame(summary_data)\n",
    "    \n",
    "    print(f\"üìà **Overall Statistics:**\")\n",
    "    print(f\"   Total stocks analyzed: {len(analysis_df)}\")\n",
    "    print(f\"   Total records: {total_records:,}\")\n",
    "    print(f\"   Average records per stock: {total_records/len(analysis_df):.0f}\")\n",
    "    print(f\"   Data efficiency: {len(collection_results)/len(target_universe)*100:.1f}%\")\n",
    "    \n",
    "    # Quality distribution\n",
    "    record_counts = [int(r) for r in analysis_df['Records']]\n",
    "    quality_tiers = {\n",
    "        'High Quality (>200 records)': sum(1 for r in record_counts if r > 200),\n",
    "        'Medium Quality (100-200 records)': sum(1 for r in record_counts if 100 <= r <= 200),\n",
    "        'Low Quality (<100 records)': sum(1 for r in record_counts if r < 100)\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nüéØ **Quality Distribution:**\")\n",
    "    for tier, count in quality_tiers.items():\n",
    "        percentage = count / len(analysis_df) * 100\n",
    "        print(f\"   {tier}: {count} stocks ({percentage:.1f}%)\")\n",
    "    \n",
    "    # Show top performers\n",
    "    analysis_df['Records_Int'] = record_counts\n",
    "    top_quality = analysis_df.nlargest(10, 'Records_Int')\n",
    "    \n",
    "    print(f\"\\nüèÜ **Top 10 Highest Quality Stocks:**\")\n",
    "    print(top_quality[['Ticker', 'Records', 'Completeness']].to_string(index=False))\n",
    "    \n",
    "    # Failed stocks analysis\n",
    "    if failed_stocks:\n",
    "        print(f\"\\n‚ö†Ô∏è **Failed Stocks Analysis:**\")\n",
    "        print(f\"   Total failed: {len(failed_stocks)}\")\n",
    "        print(f\"   Failure rate: {len(failed_stocks)/len(target_universe)*100:.1f}%\")\n",
    "        print(f\"   Sample failed: {', '.join(failed_stocks[:10])}\")\n",
    "        \n",
    "else:\n",
    "    print(\"‚ùå No data available for analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## üíæ Enterprise Data Management\n",
    "\n",
    "Save data with enterprise-grade organization and metadata:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enterprise data management\n",
    "if collection_results:\n",
    "    print(\"üíæ **Enterprise Data Management**\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Create results manager\n",
    "    results_manager = ResultsManager()\n",
    "    \n",
    "    # Generate comprehensive metadata\n",
    "    collection_metadata = {\n",
    "        'collection_profile': SELECTED_PROFILE,\n",
    "        'configuration': config,\n",
    "        'collection_statistics': {\n",
    "            'total_targeted': len(target_universe),\n",
    "            'successfully_collected': len(collection_results),\n",
    "            'failed_stocks': len(failed_stocks),\n",
    "            'success_rate': len(collection_results)/len(target_universe)*100,\n",
    "            'total_records': sum(len(data) for data in collection_results.values() if data is not None),\n",
    "            'collection_time_minutes': total_time/60,\n",
    "            'performance_stocks_per_minute': len(collection_results)/(total_time/60)\n",
    "        },\n",
    "        'date_range': {\n",
    "            'start_date': start_date,\n",
    "            'end_date': end_date,\n",
    "            'collection_date': datetime.now().isoformat()\n",
    "        },\n",
    "        'system_info': {\n",
    "            'memory_used_gb': memory_used,\n",
    "            'cpu_cores': psutil.cpu_count(),\n",
    "            'parallel_workers': config['max_workers']\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Save with enterprise structure\n",
    "        saved_files = results_manager.save_enterprise_collection(\n",
    "            stock_data=collection_results,\n",
    "            metadata=collection_metadata,\n",
    "            base_name=f\"advanced_collection_{SELECTED_PROFILE}_{datetime.now().strftime('%Y%m%d_%H%M')}\"\n",
    "        )\n",
    "        \n",
    "        print(\"‚úÖ **Enterprise save completed!**\")\n",
    "        print(f\"üìÅ **Generated files:**\")\n",
    "        for file_info in saved_files:\n",
    "            print(f\"   ‚Ä¢ {file_info['path']} ({file_info['size']} MB)\")\n",
    "            \n",
    "        # Save checkpoint for future resume\n",
    "        if config['save_checkpoints']:\n",
    "            checkpoint_file = f\"checkpoint_{SELECTED_PROFILE}_{datetime.now().strftime('%Y%m%d_%H%M')}.json\"\n",
    "            with open(checkpoint_file, 'w') as f:\n",
    "                import json\n",
    "                json.dump(checkpoint_data, f, indent=2)\n",
    "            print(f\"   ‚Ä¢ {checkpoint_file} (checkpoint)\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå **Save error:** {e}\")\n",
    "        print(\"Data remains available in memory as 'collection_results'\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No data to save\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "## üìà Performance Analysis & Optimization\n",
    "\n",
    "Analyze collection performance and provide optimization recommendations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance analysis and optimization recommendations\n",
    "print(\"üìà **Performance Analysis & Optimization**\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if collection_results:\n",
    "    # Calculate performance metrics\n",
    "    stocks_per_minute = len(collection_results) / (total_time / 60)\n",
    "    records_per_minute = total_records / (total_time / 60)\n",
    "    efficiency_score = len(collection_results) / len(target_universe) * 100\n",
    "    \n",
    "    print(f\"‚ö° **Performance Metrics:**\")\n",
    "    print(f\"   Stocks per minute: {stocks_per_minute:.1f}\")\n",
    "    print(f\"   Records per minute: {records_per_minute:,.0f}\")\n",
    "    print(f\"   Collection efficiency: {efficiency_score:.1f}%\")\n",
    "    print(f\"   Memory efficiency: {memory_used/len(collection_results):.3f} GB per stock\")\n",
    "    \n",
    "    # Performance tier assessment\n",
    "    if stocks_per_minute > 20:\n",
    "        perf_tier = \"üöÄ Excellent\"\n",
    "    elif stocks_per_minute > 10:\n",
    "        perf_tier = \"‚úÖ Good\"\n",
    "    elif stocks_per_minute > 5:\n",
    "        perf_tier = \"‚ö†Ô∏è Moderate\"\n",
    "    else:\n",
    "        perf_tier = \"‚ùå Slow\"\n",
    "    \n",
    "    print(f\"   Overall performance: {perf_tier}\")\n",
    "    \n",
    "    # Optimization recommendations\n",
    "    print(f\"\\nüéØ **Optimization Recommendations:**\")\n",
    "    \n",
    "    if efficiency_score < 90:\n",
    "        print(\"   üîß Increase max_retries for better success rate\")\n",
    "        print(\"   üîß Add delay_between_requests to reduce API errors\")\n",
    "    \n",
    "    if stocks_per_minute < 10:\n",
    "        print(\"   ‚ö° Consider reducing delay_between_batches\")\n",
    "        print(\"   ‚ö° Increase max_workers (if system resources allow)\")\n",
    "    \n",
    "    if memory_used > 8:\n",
    "        print(\"   üíæ Enable memory cleanup between batches\")\n",
    "        print(\"   üíæ Reduce checkpoint_every for more frequent cleanup\")\n",
    "    \n",
    "    if len(failed_stocks) > len(target_universe) * 0.1:\n",
    "        print(\"   üõ°Ô∏è Implement retry logic for failed stocks\")\n",
    "        print(\"   üõ°Ô∏è Add exponential backoff for rate limiting\")\n",
    "    \n",
    "    # Scaling recommendations\n",
    "    print(f\"\\nüìä **Scaling Recommendations:**\")\n",
    "    \n",
    "    if SELECTED_PROFILE == 'intermediate' and efficiency_score > 85:\n",
    "        print(\"   üöÄ Ready to upgrade to 'advanced' profile\")\n",
    "        print(\"   üöÄ Can handle 300+ stocks efficiently\")\n",
    "    \n",
    "    if SELECTED_PROFILE == 'advanced' and efficiency_score > 85:\n",
    "        print(\"   üè¢ Ready for 'enterprise' profile\")\n",
    "        print(\"   üè¢ Can scale to 1000+ stocks\")\n",
    "    \n",
    "    # Next steps\n",
    "    print(f\"\\n‚úÖ **Next Steps:**\")\n",
    "    print(\"   1. Use collected data in '04_feature_extraction.ipynb'\")\n",
    "    print(\"   2. Scale to pattern recognition with '05_pattern_model_training.ipynb'\")\n",
    "    print(\"   3. Analyze patterns with '07_pattern_match_visualization.ipynb'\")\n",
    "    print(\"   4. Set up monitoring with '08_signal_outcome_tagging.ipynb'\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ùå No performance data available\")\n",
    "\n",
    "print(f\"\\nüìÖ **Advanced collection completed:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "---\n",
    "**üöÄ Advanced Hong Kong Stock Data Collection**  \n",
    "*Enterprise-grade bulk collection with parallel processing - 100+ stocks* "
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "ipynb,py:percent",
   "notebook_metadata_filter": "all,-language_info,-toc,-latex_envs"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
