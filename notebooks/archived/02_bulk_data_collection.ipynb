{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Bulk Hong Kong Stock Data Collection\n",
    "\n",
    "This notebook demonstrates how to fetch data for **ALL Hong Kong stocks** efficiently using various approaches:\n",
    "\n",
    "## 📋 What You'll Learn\n",
    "\n",
    "1. **🎯 Curated Stock Lists**: Get major HK stocks by sector\n",
    "2. **🔍 Stock Universe Discovery**: Find all available HK stocks\n",
    "3. **⚡ Bulk Data Fetching**: Efficiently fetch large numbers of stocks\n",
    "4. **🛡️ Rate Limiting & Error Handling**: Respect API limits\n",
    "5. **📊 Progress Tracking**: Monitor large operations\n",
    "6. **💾 Data Management**: Save and organize bulk data\n",
    "\n",
    "## 🚀 Approaches Covered\n",
    "\n",
    "- **Static Lists**: Curated major stocks (most reliable)\n",
    "- **Sector-Based**: Tech, Finance, Property stocks\n",
    "- **Batch Processing**: Handle 100+ stocks efficiently\n",
    "- **Parallel Processing**: Speed up with threading (use carefully)\n",
    "\n",
    "## 📝 Note\n",
    "Each cell in this notebook can be run independently after the setup cells (1-3).\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 🔧 Setup - Run These Cells First\n",
    "\n",
    "**Important**: Run cells 1-3 before running any demonstration cells.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Cell 1: Import all required modules\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Add src directory to Python path\n",
    "notebook_dir = Path().absolute()\n",
    "src_dir = notebook_dir.parent / \"src\"\n",
    "sys.path.insert(0, str(src_dir))\n",
    "\n",
    "print(\"✅ Standard libraries imported\")\n",
    "print(f\"📁 Source path added: {src_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Cell 2: Import custom modules\n",
    "try:\n",
    "    from bulk_data_fetcher import (create_bulk_fetch_summary,\n",
    "                                   fetch_all_major_hk_stocks,\n",
    "                                   fetch_hk_stocks_bulk, fetch_hk_tech_stocks,\n",
    "                                   fetch_top_50_hk_stocks, save_bulk_data)\n",
    "    from hk_stock_universe import (MAJOR_HK_STOCKS,\n",
    "                                   get_comprehensive_hk_stock_list,\n",
    "                                   get_hk_stock_list_static,\n",
    "                                   get_hk_stocks_by_sector, get_top_hk_stocks)\n",
    "\n",
    "    print(\"✅ Custom modules imported successfully!\")\n",
    "\n",
    "except ImportError as e:\n",
    "    print(f\"❌ Import error: {e}\")\n",
    "    print(f\"Current working directory: {os.getcwd()}\")\n",
    "    print(\"Make sure you're running from the notebooks/ directory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Cell 3: Global setup - date range for all demonstrations\n",
    "end_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "start_date = (datetime.now() - timedelta(days=180)).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "print(f\"🗓️  Global date range: {start_date} to {end_date}\")\n",
    "print(f\"📊 Expected trading days: ~65\")\n",
    "print(\"🚀 Setup complete! You can now run any demonstration cell independently.\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 📊 Method 1: Explore Stock Categories\n",
    "\n",
    "**Objective**: Understand available HK stock categories and get familiar with the data structure.\n",
    "\n",
    "*This cell is completely independent - just run it!*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# METHOD 1: Explore Available Stock Categories\n",
    "print(\"📊 Available Stock Categories:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for sector, stocks in MAJOR_HK_STOCKS.items():\n",
    "    print(f\"🏢 {sector.upper()}: {len(stocks)} stocks\")\n",
    "    print(f\"   Examples: {', '.join(stocks[:3])}...\")\n",
    "    print()\n",
    "\n",
    "# Get all major stocks (deduplicated)\n",
    "all_major_stocks = get_hk_stock_list_static()\n",
    "print(f\"📈 Total unique major stocks: {len(all_major_stocks)}\")\n",
    "print(f\"🔍 Sample tickers: {all_major_stocks[:10]}\")\n",
    "\n",
    "# Demo: Get stocks by specific sector\n",
    "tech_stocks = get_hk_stocks_by_sector(\"tech_stocks\")\n",
    "finance_stocks = get_hk_stocks_by_sector(\"finance\")\n",
    "print(f\"\\n💻 Tech sector: {len(tech_stocks)} stocks\")\n",
    "print(f\"🏦 Finance sector: {len(finance_stocks)} stocks\")\n",
    "\n",
    "print(\"\\n✅ Method 1 Complete: Stock categories explored!\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## ⚡ Method 2: Bulk Fetch with Smart Batching\n",
    "\n",
    "**Objective**: Fetch multiple stocks efficiently using batch processing and rate limiting.\n",
    "\n",
    "*Independent cell - fetches 10 major stocks with progress tracking*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# METHOD 2: Bulk Fetch with Smart Batching\n",
    "print(\"🚀 DEMO: Fetching top 10 HK stocks with smart batching...\")\n",
    "\n",
    "# Fetch top 10 stocks using the built-in bulk function\n",
    "demo_stocks = fetch_all_major_hk_stocks(\n",
    "    start_date=start_date,\n",
    "    end_date=end_date,\n",
    "    max_stocks=10,  # Small demo size\n",
    "    batch_size=5,  # Process 5 at a time\n",
    "    delay_between_batches=1.0,  # 1 second between batches\n",
    ")\n",
    "\n",
    "print(f\"\\n✅ Demo completed! Fetched {len(demo_stocks)} stocks\")\n",
    "\n",
    "# Show summary of fetched data\n",
    "if demo_stocks:\n",
    "    summary_df = create_bulk_fetch_summary(demo_stocks)\n",
    "    print(\"\\n📊 Summary of fetched stocks:\")\n",
    "    display(summary_df.head())\n",
    "\n",
    "    # Show data sample for first stock\n",
    "    first_stock = list(demo_stocks.keys())[0]\n",
    "    first_data = demo_stocks[first_stock]\n",
    "    print(f\"\\n📈 Sample data for {first_stock}:\")\n",
    "    print(f\"   Records: {len(first_data)}\")\n",
    "    print(f\"   Date range: {first_data.index[0]} to {first_data.index[-1]}\")\n",
    "    print(f\"   Columns: {list(first_data.columns)}\")\n",
    "\n",
    "print(\"\\n✅ Method 2 Complete: Bulk fetching demonstrated!\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 🏢 Method 3: Sector-Specific Bulk Fetching\n",
    "\n",
    "**Objective**: Fetch stocks from specific sectors (Tech, Finance, etc.) for targeted analysis.\n",
    "\n",
    "*Independent cell - fetches tech and finance sector stocks*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# METHOD 3: Sector-Specific Bulk Fetching\n",
    "print(\"🏢 Fetching stocks by sector...\")\n",
    "\n",
    "# Fetch tech sector stocks\n",
    "print(\"\\n💻 Fetching Tech sector stocks...\")\n",
    "tech_data = fetch_hk_tech_stocks(\n",
    "    start_date=start_date, end_date=end_date, batch_size=3, delay_between_batches=1.0\n",
    ")\n",
    "print(f\"✅ Fetched {len(tech_data)} tech stocks\")\n",
    "\n",
    "# Fetch finance sector using manual approach\n",
    "print(\"\\n🏦 Fetching Finance sector stocks...\")\n",
    "finance_stocks = get_hk_stocks_by_sector(\"finance\")\n",
    "print(f\"📊 Finance sector stocks: {finance_stocks}\")\n",
    "\n",
    "# Fetch first 3 finance stocks\n",
    "finance_data = fetch_hk_stocks_bulk(\n",
    "    tickers=finance_stocks[:3],\n",
    "    start_date=start_date,\n",
    "    end_date=end_date,\n",
    "    batch_size=2,\n",
    "    delay_between_batches=1.0,\n",
    ")\n",
    "print(f\"✅ Fetched {len(finance_data)} finance stocks\")\n",
    "\n",
    "# Compare sector performance\n",
    "print(f\"\\n📊 Sector Comparison:\")\n",
    "print(f\"   💻 Tech stocks fetched: {len(tech_data)}\")\n",
    "print(f\"   🏦 Finance stocks fetched: {len(finance_data)}\")\n",
    "\n",
    "print(\"\\n✅ Method 3 Complete: Sector-specific fetching demonstrated!\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 🇭🇰 Method 4: Discover ALL Hong Kong Stocks\n",
    "\n",
    "**Objective**: Discover and analyze the complete HK stock universe (discovery only, no data fetching).\n",
    "\n",
    "*Independent cell - maps the full HK stock landscape*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# METHOD 4: Discover ALL Hong Kong Stocks\n",
    "print(\"🔍 Discovering the complete HK stock universe...\")\n",
    "\n",
    "# Get comprehensive stock universe\n",
    "stock_universe = get_comprehensive_hk_stock_list(\n",
    "    include_major=True,\n",
    "    validate_tickers=True,\n",
    "    max_tickers=500,  # Limit for demo - increase for full universe\n",
    ")\n",
    "\n",
    "# Extract and analyze the data\n",
    "all_hk_stocks = sorted(stock_universe[\"valid_stocks\"])\n",
    "print(f\"📊 Stock Universe Analysis:\")\n",
    "print(f\"   Total discovered: {len(all_hk_stocks)} stocks\")\n",
    "print(f\"   ✅ Valid: {len(stock_universe['valid_stocks'])}\")\n",
    "print(f\"   ❌ Invalid: {len(stock_universe['invalid_stocks'])}\")\n",
    "\n",
    "if all_hk_stocks:\n",
    "    print(f\"\\n🔍 Sample valid tickers: {all_hk_stocks[:10]}\")\n",
    "    print(f\"📈 Stock code range: {all_hk_stocks[0]} to {all_hk_stocks[-1]}\")\n",
    "\n",
    "# Show detailed summary\n",
    "print(f\"\\n📊 Detailed Summary:\")\n",
    "for key, value in stock_universe[\"summary\"].items():\n",
    "    print(f\"   {key}: {value}\")\n",
    "\n",
    "# Calculate theoretical full market estimates\n",
    "if len(all_hk_stocks) > 0:\n",
    "    estimated_full_universe = len(all_hk_stocks) * 20  # Rough extrapolation\n",
    "    estimated_time_hours = estimated_full_universe * 1.5 / 3600\n",
    "    estimated_data_gb = estimated_full_universe * 0.001\n",
    "\n",
    "    print(f\"\\n🚨 Theoretical FULL Market Estimates:\")\n",
    "    print(f\"   📊 Estimated total HK stocks: ~{estimated_full_universe:,}\")\n",
    "    print(f\"   ⏱️  Estimated fetch time: ~{estimated_time_hours:.1f} hours\")\n",
    "    print(f\"   💾 Estimated data size: ~{estimated_data_gb:.1f} GB\")\n",
    "    print(f\"   🌐 Estimated API calls: ~{estimated_full_universe:,}\")\n",
    "\n",
    "print(f\"\\n⚠️  Note: This demo uses a limited subset for demonstration.\")\n",
    "print(f\"💡 To discover the full universe, increase max_tickers parameter.\")\n",
    "\n",
    "print(\"\\n✅ Method 4 Complete: Stock universe discovery demonstrated!\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 🚀 Method 5: Fetch ALL Hong Kong Stocks (Full Universe)\n",
    "\n",
    "**Objective**: Actually fetch stock data for ALL discovered HK stocks with progress tracking.\n",
    "\n",
    "*Independent cell - WARNING: This will take significant time and API quota!*\n",
    "\n",
    "**⚠️ CRITICAL**: This fetches data for ALL stocks. Use with caution!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# METHOD 5: Fetch ALL Hong Kong Stocks (Complete Universe Approach)\n",
    "print(\"🚀 COMPREHENSIVE HK MARKET FETCH\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Safety configuration - CHANGE TO True TO EXECUTE FULL FETCH\n",
    "EXECUTE_FULL_FETCH = False  # ⚠️ Change to True to execute full universe fetch\n",
    "DEMO_SIZE = 50              # Number of stocks for demo mode\n",
    "\n",
    "print(f\"🛡️  Safety mode: {'DISABLED' if EXECUTE_FULL_FETCH else 'ENABLED'}\")\n",
    "print(f\"📊 Demo size: {DEMO_SIZE} stocks\")\n",
    "\n",
    "if not EXECUTE_FULL_FETCH:\n",
    "    print(\"\n",
    "💡 To enable FULL universe fetch:\")\n",
    "    print(\"   1. Set EXECUTE_FULL_FETCH = True\")\n",
    "    print(\"   2. Ensure sufficient API quota (1000+ calls)\")\n",
    "    print(\"   3. Prepare for 2-6 hours execution time\")\n",
    "    print(\"   4. Monitor system resources\")\n",
    "    \n",
    "    # Demo with subset of discovered stocks\n",
    "    print(f\"\n",
    "🎯 DEMO: Fetching {DEMO_SIZE} stocks from discovered universe...\")\n",
    "    \n",
    "    # Get stock universe for demo\n",
    "    demo_universe = get_comprehensive_hk_stock_list(\n",
    "        include_major=True,\n",
    "        validate_tickers=True,\n",
    "        max_tickers=DEMO_SIZE\n",
    "    )\n",
    "    \n",
    "    demo_stocks_list = sorted(demo_universe['valid_stocks'])[:DEMO_SIZE]\n",
    "    \n",
    "    try:\n",
    "        demo_comprehensive_data = fetch_hk_stocks_bulk(\n",
    "            tickers=demo_stocks_list,\n",
    "            start_date=start_date,\n",
    "            end_date=end_date,\n",
    "            batch_size=10,\n",
    "            delay_between_batches=1.0\n",
    "        )\n",
    "        \n",
    "        print(f\"✅ Demo completed successfully!\")\n",
    "        print(f\"📊 Fetched: {len(demo_comprehensive_data)} out of {len(demo_stocks_list)} stocks\")\n",
    "        print(f\"📈 Demo success rate: {len(demo_comprehensive_data)/len(demo_stocks_list)*100:.1f}%\")\n",
    "        \n",
    "        if demo_comprehensive_data:\n",
    "            print(f\"🌟 Sample fetched stocks: {list(demo_comprehensive_data.keys())[:5]}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Demo failed: {e}\")\n",
    "        \n",
    "else:\n",
    "    print(\"🚨 EXECUTING FULL HK UNIVERSE FETCH!\")\n",
    "    print(\"⚠️  WARNING: This will take HOURS and significant API quota!\")\n",
    "    \n",
    "    # Get complete HK stock universe\n",
    "    print(\"🔍 Discovering complete HK stock universe...\")\n",
    "    full_universe = get_comprehensive_hk_stock_list(\n",
    "        include_major=True,\n",
    "        validate_tickers=True,\n",
    "        max_tickers=None  # Get ALL available stocks\n",
    "    )\n",
    "    \n",
    "    all_discovered_stocks = sorted(full_universe['valid_stocks'])\n",
    "    print(f\"🎯 Target: {len(all_discovered_stocks)} total discovered HK stocks\")\n",
    "    \n",
    "    # Execute comprehensive fetch with progress tracking\n",
    "    def fetch_universe_with_progress(stock_list, batch_size=25):\n",
    "        successful_data = {}\n",
    "        failed_stocks = []\n",
    "        \n",
    "        total_batches = (len(stock_list) + batch_size - 1) // batch_size\n",
    "        print(f\"📦 Processing {total_batches} batches of up to {batch_size} stocks each\")\n",
    "        \n",
    "        for batch_num in range(total_batches):\n",
    "            start_idx = batch_num * batch_size\n",
    "            end_idx = min(start_idx + batch_size, len(stock_list))\n",
    "            batch_stocks = stock_list[start_idx:end_idx]\n",
    "            \n",
    "            print(f\"\n",
    "📦 Batch {batch_num+1}/{total_batches}: {batch_stocks[0]} to {batch_stocks[-1]}\")\n",
    "            \n",
    "            try:\n",
    "                batch_data = fetch_hk_stocks_bulk(\n",
    "                    tickers=batch_stocks,\n",
    "                    start_date=start_date,\n",
    "                    end_date=end_date,\n",
    "                    batch_size=10,\n",
    "                    delay_between_batches=2.0\n",
    "                )\n",
    "                \n",
    "                successful_data.update(batch_data)\n",
    "                print(f\"   ✅ Batch success: {len(batch_data)} stocks fetched\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"   ❌ Batch failed: {e}\")\n",
    "                failed_stocks.extend(batch_stocks)\n",
    "            \n",
    "            # Progress update\n",
    "            progress = (batch_num + 1) / total_batches * 100\n",
    "            print(f\"   📈 Overall progress: {progress:.1f}% - Total fetched: {len(successful_data)}\")\n",
    "        \n",
    "        return successful_data, failed_stocks\n",
    "    \n",
    "    # Execute the complete universe fetch\n",
    "    universe_data, universe_failures = fetch_universe_with_progress(\n",
    "        all_discovered_stocks,\n",
    "        batch_size=25\n",
    "    )\n",
    "    \n",
    "    # Save complete universe data\n",
    "    if universe_data:\n",
    "        print(f\"\n",
    "💾 Saving complete HK universe dataset...\")\n",
    "        save_bulk_data(\n",
    "            stock_data=universe_data,\n",
    "            base_dir=\"data/complete_hk_universe\"\n",
    "        )\n",
    "    \n",
    "    print(f\"\n",
    "🎉 COMPLETE HK UNIVERSE FETCH FINISHED!\")\n",
    "    print(f\"✅ Successfully fetched: {len(universe_data)} stocks\")\n",
    "    print(f\"❌ Failed: {len(universe_failures)} stocks\")\n",
    "         print(f\"📈 Success rate: {len(universe_data)/len(all_discovered_stocks)*100:.1f}%\")\n",
    "\n",
    "print(\"\n",
    "✅ Method 5 Complete: Comprehensive HK market fetch capability delivered!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 💾 Method 6: Data Management & Saving\n",
    "\n",
    "**Objective**: Save and organize bulk-fetched data systematically for future analysis.\n",
    "\n",
    "*Independent cell - demonstrates saving and data management*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# METHOD 6: Data Management & Saving\n",
    "print(\"💾 Demonstrating data saving and management...\")\n",
    "\n",
    "# First, get some sample data\n",
    "print(\"📊 Fetching sample data for saving demo...\")\n",
    "sample_stocks = fetch_all_major_hk_stocks(\n",
    "    start_date=start_date,\n",
    "    end_date=end_date,\n",
    "    max_stocks=5,  # Small sample\n",
    "    batch_size=3,\n",
    "    delay_between_batches=0.5,\n",
    ")\n",
    "\n",
    "if sample_stocks:\n",
    "    print(f\"✅ Got {len(sample_stocks)} stocks for saving demo\")\n",
    "\n",
    "    # Create and display summary\n",
    "    summary_df = create_bulk_fetch_summary(sample_stocks)\n",
    "    print(\"\\n📊 Data Summary before saving:\")\n",
    "    display(summary_df)\n",
    "\n",
    "    # Save bulk data\n",
    "    print(\"\\n💾 Saving data to files...\")\n",
    "    try:\n",
    "        saved_files = save_bulk_data(\n",
    "            stock_data=sample_stocks, base_dir=\"data/demo_bulk_save\"\n",
    "        )\n",
    "        print(f\"✅ Successfully saved data!\")\n",
    "        print(f\"📁 Files saved in: data/demo_bulk_save/\")\n",
    "\n",
    "        # Show what was saved\n",
    "        if \"files\" in saved_files:\n",
    "            print(f\"📄 Individual stock files: {len(saved_files['files'])}\")\n",
    "        if \"summary_file\" in saved_files:\n",
    "            print(f\"📊 Summary file: {saved_files['summary_file']}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️  Saving failed: {e}\")\n",
    "        print(\"💡 This is normal if data directory doesn't exist\")\n",
    "\n",
    "    # Demonstrate manual saving approach\n",
    "    print(f\"\\n📝 Manual saving approach:\")\n",
    "    for ticker, data in list(sample_stocks.items())[:2]:  # First 2 stocks\n",
    "        filename = f\"manual_{ticker.replace('.', '_')}.csv\"\n",
    "        print(f\"   💾 Would save {ticker} to {filename} ({len(data)} records)\")\n",
    "\n",
    "else:\n",
    "    print(\"❌ No sample data available for saving demo\")\n",
    "\n",
    "print(\"\\n✅ Method 6 Complete: Data management demonstrated!\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 🛡️ Method 7: Error Handling & Retry Logic\n",
    "\n",
    "**Objective**: Demonstrate robust error handling with retry logic for production-ready bulk fetching.\n",
    "\n",
    "*Independent cell - simulates failures and shows recovery strategies*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# METHOD 7: Error Handling & Retry Logic\n",
    "print(\"🛡️ Demonstrating robust error handling...\")\n",
    "\n",
    "# Define a robust fetching function with retry logic\n",
    "\n",
    "\n",
    "def demo_robust_fetch(stock_list, max_retries=3):\n",
    "    \"\"\"Demo function showing retry logic and error handling\"\"\"\n",
    "    successful_fetches = {}\n",
    "    failed_stocks = []\n",
    "\n",
    "    for stock in stock_list:\n",
    "        retry_count = 0\n",
    "        success = False\n",
    "\n",
    "        while retry_count < max_retries and not success:\n",
    "            try:\n",
    "                print(f\"🔄 Fetching {stock} (attempt {retry_count + 1}/{max_retries})\")\n",
    "\n",
    "                # Simulate API call with 70% success rate\n",
    "                if random.random() < 0.7:\n",
    "                    successful_fetches[stock] = f\"✅ Data for {stock}\"\n",
    "                    success = True\n",
    "                    print(f\"   ✅ Success!\")\n",
    "                else:\n",
    "                    raise Exception(\"Simulated API timeout\")\n",
    "\n",
    "            except Exception as e:\n",
    "                retry_count += 1\n",
    "                print(f\"   ⚠️  Failed: {e}\")\n",
    "\n",
    "                if retry_count < max_retries:\n",
    "                    wait_time = retry_count * 1  # Progressive backoff\n",
    "                    print(f\"   ⏱️  Waiting {wait_time}s before retry...\")\n",
    "                    time.sleep(wait_time)\n",
    "                else:\n",
    "                    failed_stocks.append(stock)\n",
    "                    print(f\"   ❌ Gave up after {max_retries} attempts\")\n",
    "\n",
    "    return successful_fetches, failed_stocks\n",
    "\n",
    "\n",
    "# Demo with sample stocks\n",
    "sample_stock_list = [\"0700.HK\", \"0005.HK\", \"0941.HK\", \"1299.HK\", \"2318.HK\"]\n",
    "print(f\"🚀 Testing robust fetching with {len(sample_stock_list)} stocks:\")\n",
    "\n",
    "# Set random seed for reproducible demo\n",
    "random.seed(42)\n",
    "\n",
    "successful_data, failed_list = demo_robust_fetch(\n",
    "    stock_list=sample_stock_list, max_retries=2\n",
    ")\n",
    "\n",
    "print(f\"\\n📊 Error Handling Results:\")\n",
    "print(f\"   ✅ Successful: {len(successful_data)} stocks\")\n",
    "print(f\"   ❌ Failed: {len(failed_list)} stocks\")\n",
    "print(f\"   📈 Success rate: {len(successful_data)/len(sample_stock_list)*100:.1f}%\")\n",
    "\n",
    "if failed_list:\n",
    "    print(f\"   🔍 Failed stocks: {failed_list}\")\n",
    "\n",
    "# Show error handling best practices\n",
    "print(f\"\\n🛡️ Error Handling Best Practices:\")\n",
    "print(\"   1. ✅ Implement exponential backoff\")\n",
    "print(\"   2. ✅ Set maximum retry limits\")\n",
    "print(\"   3. ✅ Log detailed error information\")\n",
    "print(\"   4. ✅ Continue processing other stocks\")\n",
    "print(\"   5. ✅ Track success/failure rates\")\n",
    "print(\"   6. ✅ Provide recovery mechanisms\")\n",
    "\n",
    "print(\"\\n✅ Method 7 Complete: Error handling demonstrated!\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## ⚡ Method 8: Parallel Processing (Advanced)\n",
    "\n",
    "**Objective**: Demonstrate parallel processing with threading for faster bulk fetching.\n",
    "\n",
    "*Independent cell - shows parallel processing with safety considerations*\n",
    "\n",
    "**⚠️ WARNING**: Use parallel processing carefully to avoid overwhelming APIs!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# METHOD 8: Parallel Processing (Advanced)\n",
    "print(\"⚡ Demonstrating parallel processing...\")\n",
    "print(\"⚠️  Using conservative settings to respect API limits\")\n",
    "\n",
    "\n",
    "def demo_fetch_single_stock(stock_symbol):\n",
    "    \"\"\"Demo function for single stock fetch with delay\"\"\"\n",
    "    try:\n",
    "        time.sleep(0.5)  # Conservative delay per request\n",
    "        print(f\"   🔄 Processed {stock_symbol}\")\n",
    "        return f\"Data for {stock_symbol}\"\n",
    "    except Exception as e:\n",
    "        print(f\"   ❌ Error with {stock_symbol}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# Test with small stock list\n",
    "demo_stocks = [\"0700.HK\", \"0005.HK\", \"0941.HK\", \"1299.HK\"]\n",
    "\n",
    "print(f\"🚀 Parallel Processing Demo ({len(demo_stocks)} stocks):\")\n",
    "print(f\"⚡ Comparing sequential vs parallel processing...\")\n",
    "\n",
    "# Sequential processing\n",
    "print(f\"\\n📈 Sequential Processing:\")\n",
    "start_time = time.time()\n",
    "sequential_results = []\n",
    "for stock in demo_stocks:\n",
    "    result = demo_fetch_single_stock(stock)\n",
    "    sequential_results.append(result)\n",
    "sequential_time = time.time() - start_time\n",
    "\n",
    "print(f\"   ⏱️  Sequential time: {sequential_time:.2f} seconds\")\n",
    "\n",
    "# Parallel processing with limited workers\n",
    "print(f\"\\n⚡ Parallel Processing (2 workers max):\")\n",
    "start_time = time.time()\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=2) as executor:  # CONSERVATIVE: Only 2 workers\n",
    "    parallel_results = list(executor.map(demo_fetch_single_stock, demo_stocks))\n",
    "\n",
    "parallel_time = time.time() - start_time\n",
    "\n",
    "print(f\"   ⏱️  Parallel time: {parallel_time:.2f} seconds\")\n",
    "print(f\"   🚀 Speed improvement: {sequential_time/parallel_time:.1f}x\")\n",
    "\n",
    "# Results comparison\n",
    "successful_sequential = len([r for r in sequential_results if r])\n",
    "successful_parallel = len([r for r in parallel_results if r])\n",
    "\n",
    "print(f\"\\n📊 Results Comparison:\")\n",
    "print(f\"   📈 Sequential: {successful_sequential}/{len(demo_stocks)} successful\")\n",
    "print(f\"   ⚡ Parallel: {successful_parallel}/{len(demo_stocks)} successful\")\n",
    "\n",
    "# Safety recommendations\n",
    "print(f\"\\n🛡️ Parallel Processing Safety Guidelines:\")\n",
    "print(\"   1. ⚠️  Start with max_workers=2 (conservative)\")\n",
    "print(\"   2. ⏱️  Include delays in individual requests\")\n",
    "print(\"   3. 📊 Monitor API response times\")\n",
    "print(\"   4. 🔄 Test with small batches first\")\n",
    "print(\"   5. 📈 Scale up gradually if successful\")\n",
    "print(\"   6. 🚨 Have fallback to sequential processing\")\n",
    "\n",
    "print(\"\\n✅ Method 8 Complete: Parallel processing demonstrated!\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 📊 Summary & Best Practices\n",
    "\n",
    "**Key takeaways and recommendations from all methods demonstrated above.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# SUMMARY: Best Practices & Recommendations\n",
    "print(\"📊 BULK HK STOCK DATA COLLECTION - SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\n🎯 RECOMMENDED APPROACHES:\")\n",
    "\n",
    "print(\"\\n1. 🔰 BEGINNER: Start Small\")\n",
    "print(\"   • Use: fetch_all_major_hk_stocks(max_stocks=10-20)\")\n",
    "print(\"   • Benefits: Safe, reliable, fast\")\n",
    "print(\"   • Best for: Learning and testing\")\n",
    "\n",
    "print(\"\\n2. 📊 SECTOR ANALYSIS: Targeted Approach\")\n",
    "print(\"   • Use: fetch_hk_tech_stocks() or get_hk_stocks_by_sector()\")\n",
    "print(\"   • Benefits: Focused analysis, manageable size\")\n",
    "print(\"   • Best for: Sector-specific research\")\n",
    "\n",
    "print(\"\\n3. 🚀 ADVANCED: Comprehensive Analysis\")\n",
    "print(\"   • Use: get_comprehensive_hk_stock_list() with batching\")\n",
    "print(\"   • Benefits: Full market coverage\")\n",
    "print(\"   • Best for: Complete market analysis\")\n",
    "\n",
    "print(\"\\n4. ⚡ ENTERPRISE: High-Volume Processing\")\n",
    "print(\"   • Use: Parallel processing with careful rate limiting\")\n",
    "print(\"   • Benefits: Faster processing\")\n",
    "print(\"   • Best for: Production systems with monitoring\")\n",
    "\n",
    "print(\"\\n🛡️ CRITICAL SUCCESS FACTORS:\")\n",
    "print(\"   ✅ Respect API rate limits (1-2 second delays)\")\n",
    "print(\"   ✅ Use batch processing (5-20 stocks per batch)\")\n",
    "print(\"   ✅ Implement retry logic with backoff\")\n",
    "print(\"   ✅ Monitor success rates and performance\")\n",
    "print(\"   ✅ Save data systematically\")\n",
    "print(\"   ✅ Test with small datasets first\")\n",
    "\n",
    "print(\"\\n⚠️  IMPORTANT WARNINGS:\")\n",
    "print(\"   🚨 Full HK market = 1000+ stocks = hours of processing\")\n",
    "print(\"   🚨 Always validate tickers before bulk processing\")\n",
    "print(\"   🚨 Monitor API usage quotas\")\n",
    "print(\"   🚨 Parallel processing can trigger rate limits\")\n",
    "\n",
    "print(\"\\n🚀 PRODUCTION READY CHECKLIST:\")\n",
    "print(\"   □ Error handling and retry logic\")\n",
    "print(\"   □ Progress tracking and logging\")\n",
    "print(\"   □ Data validation and quality checks\")\n",
    "print(\"   □ Checkpoint/resume capability\")\n",
    "print(\"   □ Resource monitoring (CPU, memory, network)\")\n",
    "print(\"   □ Fallback strategies for failures\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"✅ NOTEBOOK COMPLETED SUCCESSFULLY!\")\n",
    "print(\"📚 You now have tools for any scale of HK stock data collection!\")\n",
    "print(\"🇭🇰 Happy analyzing! 📈\")\n",
    "\n",
    "# Show what was accomplished\n",
    "methods_completed = [\n",
    "    \"✅ Method 1: Stock categories exploration\",\n",
    "    \"✅ Method 2: Smart batching demonstration\",\n",
    "    \"✅ Method 3: Sector-specific fetching\",\n",
    "    \"✅ Method 4: Full HK stock universe discovery\",\n",
    "    \"✅ Method 5: Comprehensive HK market fetch (full universe)\",\n",
    "    \"✅ Method 6: Data management and saving\",\n",
    "    \"✅ Method 7: Error handling and retry logic\",\n",
    "    \"✅ Method 8: Parallel processing (advanced)\",\n",
    "]\n",
    "\n",
    "print(f\"\\n📋 METHODS DEMONSTRATED:\")\n",
    "for method in methods_completed:\n",
    "    print(f\"   {method}\")\n",
    "\n",
    "print(f\"\\n💡 NEXT STEPS:\")\n",
    "print(\"   • Run individual cells based on your needs\")\n",
    "print(\"   • Modify parameters for your specific requirements\")\n",
    "print(\"   • Scale up gradually from small to large datasets\")\n",
    "print(\"   • Implement production safeguards for real applications\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Add src directory to Python path\n",
    "notebook_dir = Path().absolute()\n",
    "src_dir = notebook_dir.parent / \"src\"\n",
    "sys.path.insert(0, str(src_dir))\n",
    "\n",
    "# Import our bulk fetching modules\n",
    "try:\n",
    "    from bulk_data_fetcher import (create_bulk_fetch_summary,\n",
    "                                   fetch_all_major_hk_stocks,\n",
    "                                   fetch_hk_stocks_bulk, fetch_hk_tech_stocks,\n",
    "                                   fetch_top_50_hk_stocks, save_bulk_data)\n",
    "    from hk_stock_universe import (MAJOR_HK_STOCKS,\n",
    "                                   get_comprehensive_hk_stock_list,\n",
    "                                   get_hk_stock_list_static,\n",
    "                                   get_hk_stocks_by_sector, get_top_hk_stocks)\n",
    "\n",
    "    print(\"✅ All modules imported successfully!\")\n",
    "    print(\"🚀 Ready for bulk data collection!\")\n",
    "except ImportError as e:\n",
    "    print(f\"❌ Import error: {e}\")\n",
    "    print(f\"Current working directory: {os.getcwd()}\")\n",
    "    print(f\"Python path: {sys.path[:3]}...\")  # Show first 3 entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up date range for all demonstrations\n",
    "end_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "start_date = (datetime.now() - timedelta(days=90)).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "print(f\"📅 Date range for all demos: {start_date} to {end_date}\")\n",
    "print(f\"📊 Expected trading days: ~65\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Helper function definitions for advanced demonstrations\n",
    "\n",
    "\n",
    "def fetch_all_hk_with_progress(stock_list, start_date, end_date, batch_size=10):\n",
    "    \"\"\"\n",
    "    Fetch all stocks with progress bar and comprehensive tracking\n",
    "    \"\"\"\n",
    "    successful_data = {}\n",
    "    failed_stocks = []\n",
    "    progress_stats = {\n",
    "        \"total_stocks\": len(stock_list),\n",
    "        \"completed\": 0,\n",
    "        \"success_rate\": 0,\n",
    "        \"start_time\": time.time(),\n",
    "    }\n",
    "\n",
    "    # Create progress bar\n",
    "    pbar = tqdm(total=len(stock_list), desc=\"Fetching HK Stocks\")\n",
    "\n",
    "    # Process in batches\n",
    "    for i in range(0, len(stock_list), batch_size):\n",
    "        batch = stock_list[i : i + batch_size]\n",
    "        pbar.set_description(f\"Batch {i//batch_size + 1} ({batch[0]} to {batch[-1]})\")\n",
    "\n",
    "        # Fetch batch using existing function\n",
    "        try:\n",
    "            batch_data = fetch_hk_stocks_bulk(\n",
    "                tickers=batch,\n",
    "                start_date=start_date,\n",
    "                end_date=end_date,\n",
    "                batch_size=batch_size,\n",
    "                delay_between_batches=1.0,\n",
    "            )\n",
    "\n",
    "            successful_data.update(batch_data)\n",
    "            pbar.update(len(batch))\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Batch failed: {e}\")\n",
    "            failed_stocks.extend(batch)\n",
    "            pbar.update(len(batch))\n",
    "\n",
    "        # Update progress stats\n",
    "        progress_stats[\"completed\"] = len(successful_data)\n",
    "        progress_stats[\"success_rate\"] = (\n",
    "            len(successful_data) / progress_stats[\"total_stocks\"] * 100\n",
    "        )\n",
    "\n",
    "        # Update progress bar with stats\n",
    "        elapsed_time = time.time() - progress_stats[\"start_time\"]\n",
    "        stocks_per_second = (\n",
    "            progress_stats[\"completed\"] / elapsed_time if elapsed_time > 0 else 0\n",
    "        )\n",
    "\n",
    "        pbar.set_postfix(\n",
    "            {\n",
    "                \"Success\": f\"{progress_stats['success_rate']:.1f}%\",\n",
    "                \"Rate\": f\"{stocks_per_second:.2f}/s\",\n",
    "            }\n",
    "        )\n",
    "\n",
    "    pbar.close()\n",
    "\n",
    "    # Final statistics\n",
    "    total_time = time.time() - progress_stats[\"start_time\"]\n",
    "    print(f\"\\n📊 COMPREHENSIVE FETCH COMPLETED!\")\n",
    "    print(f\"✅ Successfully fetched: {len(successful_data)} stocks\")\n",
    "    print(f\"❌ Failed: {len(failed_stocks)} stocks\")\n",
    "    print(f\"📈 Success rate: {len(successful_data)/len(stock_list)*100:.1f}%\")\n",
    "    print(f\"⏱️  Total time: {total_time/60:.1f} minutes\")\n",
    "    print(f\"🚀 Average rate: {len(successful_data)/total_time:.2f} stocks/second\")\n",
    "\n",
    "    return successful_data, failed_stocks\n",
    "\n",
    "\n",
    "def robust_bulk_fetch(stock_list, start_date, end_date, max_retries=3):\n",
    "    \"\"\"\n",
    "    Fetch stocks with retry logic and error handling\n",
    "    \"\"\"\n",
    "    successful_fetches = {}\n",
    "    failed_stocks = []\n",
    "\n",
    "    for stock in stock_list:\n",
    "        retry_count = 0\n",
    "        success = False\n",
    "\n",
    "        while retry_count < max_retries and not success:\n",
    "            try:\n",
    "                print(f\"🔄 Fetching {stock} (attempt {retry_count + 1}/{max_retries})\")\n",
    "\n",
    "                # Simulate API call with potential failure\n",
    "                if random.random() < 0.8:  # 80% success rate for demo\n",
    "                    # This would be your actual data fetching call\n",
    "                    successful_fetches[stock] = f\"Mock data for {stock}\"\n",
    "                    success = True\n",
    "                    print(f\"✅ Successfully fetched {stock}\")\n",
    "                else:\n",
    "                    raise Exception(\"Simulated API error\")\n",
    "\n",
    "            except Exception as e:\n",
    "                retry_count += 1\n",
    "                print(f\"⚠️  Error fetching {stock}: {e}\")\n",
    "\n",
    "                if retry_count < max_retries:\n",
    "                    wait_time = retry_count * 2  # Exponential backoff\n",
    "                    print(f\"⏱️  Waiting {wait_time} seconds before retry...\")\n",
    "                    time.sleep(wait_time)\n",
    "                else:\n",
    "                    failed_stocks.append(stock)\n",
    "                    print(f\"❌ Failed to fetch {stock} after {max_retries} attempts\")\n",
    "\n",
    "    return successful_fetches, failed_stocks\n",
    "\n",
    "\n",
    "def fetch_single_stock_with_delay(stock_symbol):\n",
    "    \"\"\"Fetch a single stock with built-in delay for parallel processing demo\"\"\"\n",
    "    try:\n",
    "        time.sleep(0.5)  # Conservative delay\n",
    "        print(f\"🔄 Fetching {stock_symbol}...\")\n",
    "        return f\"Data for {stock_symbol}\"\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error fetching {stock_symbol}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "print(\"✅ Helper functions defined!\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Method 1: Fetch Major HK Stocks by Sector\n",
    "\n",
    "Start with curated lists of major Hong Kong stocks organized by sector. This is the most reliable approach.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Explore available stock categories\n",
    "print(\"📊 Available Stock Categories:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for sector, stocks in MAJOR_HK_STOCKS.items():\n",
    "    print(f\"🏢 {sector.upper()}: {len(stocks)} stocks\")\n",
    "    print(f\"   Examples: {', '.join(stocks[:3])}...\")\n",
    "    print()\n",
    "\n",
    "# Get all major stocks (deduplicated)\n",
    "all_major_stocks = get_hk_stock_list_static()\n",
    "print(f\"📈 Total unique major stocks: {len(all_major_stocks)}\")\n",
    "\n",
    "# Show some examples\n",
    "print(f\"🔍 Sample tickers: {all_major_stocks[:10]}\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Method 2: Bulk Fetch with Smart Batching\n",
    "\n",
    "Now let's fetch data for all major stocks using intelligent batching and rate limiting.\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "len(all_major_stocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Set up date range (last 3 months for demo)\n",
    "end_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "start_date = (datetime.now() - timedelta(days=180)).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "print(f\"📅 Fetching data from {start_date} to {end_date}\")\n",
    "print(f\"📊 Expected trading days: ~65\")\n",
    "\n",
    "# Example 1: Fetch top 20 stocks (for demo purposes)\n",
    "print(\"\\n🚀 DEMO: Fetching top 20 HK stocks...\")\n",
    "\n",
    "demo_stocks = fetch_all_major_hk_stocks(\n",
    "    start_date=start_date,\n",
    "    end_date=end_date,\n",
    "    max_stocks=20,  # Limit for demo\n",
    "    batch_size=5,  # Small batches for demo\n",
    "    delay_between_batches=1.0,  # 1 second between batches\n",
    ")\n",
    "\n",
    "print(f\"\\n✅ Demo completed! Fetched {len(demo_stocks)} stocks\")\n",
    "\n",
    "# Show summary\n",
    "if demo_stocks:\n",
    "    summary_df = create_bulk_fetch_summary(demo_stocks)\n",
    "    print(\"\\n📊 Summary of fetched stocks:\")\n",
    "    display(summary_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Method 3: Sector-Specific Bulk Fetching\n",
    "\n",
    "Focus on specific sectors for targeted analysis. This is useful when you want to analyze particular market segments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Example: Fetch all Tech stocks\n",
    "print(\"🚀 Fetching Tech sector stocks...\")\n",
    "tech_stocks = fetch_hk_tech_stocks(\n",
    "    start_date=start_date, end_date=end_date, delay_between_requests=0.5\n",
    ")\n",
    "\n",
    "print(f\"✅ Fetched {len(tech_stocks)} tech stocks\")\n",
    "\n",
    "# Example: Fetch Finance sector stocks\n",
    "print(\"\\n🏦 Fetching Finance sector stocks...\")\n",
    "finance_stocks = get_hk_stocks_by_sector(\"finance\")\n",
    "print(f\"📊 Finance sector stocks: {finance_stocks}\")\n",
    "\n",
    "# Demonstrate batch processing for a specific sector\n",
    "finance_data = fetch_hk_stocks_bulk(\n",
    "    tickers=finance_stocks[:5],  # First 5 finance stocks\n",
    "    start_date=start_date,\n",
    "    end_date=end_date,\n",
    "    batch_size=3,\n",
    "    delay_between_batches=1.0,\n",
    ")\n",
    "\n",
    "print(f\"\\n📈 Successfully fetched data for {len(finance_data)} finance stocks\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Step 1: Get comprehensive list of ALL HK stocks\n",
    "print(\"🔍 Discovering ALL Hong Kong stocks...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Get the complete HK stock universe\n",
    "stock_universe = get_comprehensive_hk_stock_list(\n",
    "    include_major=True,\n",
    "    validate_tickers=True,\n",
    "    max_tickers=100,  # Limit for demo - increase for full universe\n",
    ")\n",
    "\n",
    "# Extract the actual stock tickers from the result\n",
    "all_hk_stocks = sorted(stock_universe[\"valid_stocks\"])\n",
    "print(f\"📊 Total HK stocks discovered: {len(all_hk_stocks)}\")\n",
    "print(f\"✅ Valid stocks: {len(stock_universe['valid_stocks'])}\")\n",
    "print(f\"❌ Invalid stocks: {len(stock_universe['invalid_stocks'])}\")\n",
    "\n",
    "# Show sample of discovered stocks\n",
    "print(f\"🔍 Sample tickers: {all_hk_stocks[:20]}\")\n",
    "if len(all_hk_stocks) > 10:\n",
    "    print(f\"📈 Stock range: {all_hk_stocks[0]} to {all_hk_stocks[-10:]}\")\n",
    "\n",
    "# Show summary\n",
    "print(f\"\\n📊 Stock Universe Summary:\")\n",
    "for key, value in stock_universe[\"summary\"].items():\n",
    "    print(f\"   {key}: {value}\")\n",
    "\n",
    "# Calculate estimated time and API calls\n",
    "estimated_time_hours = len(all_hk_stocks) * 1.5 / 3600  # 1.5 seconds per stock\n",
    "estimated_api_calls = len(all_hk_stocks)\n",
    "\n",
    "print(f\"\\n⏱️  Estimated time for ALL stocks: {estimated_time_hours:.1f} hours\")\n",
    "print(f\"🌐 Estimated API calls: {estimated_api_calls:,}\")\n",
    "print(f\"💰 Estimated data size: ~{len(all_hk_stocks) * 0.1:.1f} MB\")\n",
    "\n",
    "print(\"\\n🚨 **RECOMMENDATION**: Start with a subset for testing!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Step 2: Smart subset selection for demonstration\n",
    "print(\"🎯 Creating manageable subsets for demonstration:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Option 1: Top N stocks by stock code (usually more liquid)\n",
    "top_100_stocks = all_hk_stocks[:100]\n",
    "print(f\"📊 Top 100 stocks (by code): {len(top_100_stocks)}\")\n",
    "\n",
    "# Option 2: Random sampling across the range\n",
    "import random\n",
    "\n",
    "random.seed(42)  # For reproducible results\n",
    "random_sample = random.sample(all_hk_stocks, min(50, len(all_hk_stocks)))\n",
    "random_sample.sort()  # Sort for easier tracking\n",
    "print(f\"🎲 Random sample: {len(random_sample)} stocks\")\n",
    "print(f\"   Examples: {random_sample[:10]}\")\n",
    "\n",
    "# Option 3: Systematic sampling (every Nth stock)\n",
    "systematic_sample = all_hk_stocks[\n",
    "    :: max(1, len(all_hk_stocks) // 50)\n",
    "]  # Every ~20th stock\n",
    "print(f\"📐 Systematic sample: {len(systematic_sample)} stocks\")\n",
    "print(f\"   Examples: {systematic_sample[:10]}\")\n",
    "\n",
    "# Choose which subset to use for demo\n",
    "DEMO_SUBSET = random_sample  # Use random sample for variety\n",
    "print(f\"\\n✅ Selected subset for demo: {len(DEMO_SUBSET)} stocks\")\n",
    "print(f\"📋 Subset range: {DEMO_SUBSET[0]} to {DEMO_SUBSET[-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Step 3: Fetch subset with progress tracking\n",
    "print(\"🚀 Fetching comprehensive HK stock data with progress tracking...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "import time\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "def fetch_all_hk_with_progress(stock_list, start_date, end_date, batch_size=10):\n",
    "    \"\"\"\n",
    "    Fetch all stocks with progress bar and comprehensive tracking\n",
    "    \"\"\"\n",
    "    successful_data = {}\n",
    "    failed_stocks = []\n",
    "    progress_stats = {\n",
    "        \"total_stocks\": len(stock_list),\n",
    "        \"completed\": 0,\n",
    "        \"success_rate\": 0,\n",
    "        \"start_time\": time.time(),\n",
    "    }\n",
    "\n",
    "    # Create progress bar\n",
    "    pbar = tqdm(total=len(stock_list), desc=\"Fetching HK Stocks\")\n",
    "\n",
    "    # Process in batches\n",
    "    for i in range(0, len(stock_list), batch_size):\n",
    "        batch = stock_list[i : i + batch_size]\n",
    "        pbar.set_description(f\"Batch {i//batch_size + 1} ({batch[0]} to {batch[-1]})\")\n",
    "\n",
    "        # Fetch batch using existing function\n",
    "        try:\n",
    "            batch_data = fetch_hk_stocks_bulk(\n",
    "                tickers=batch,\n",
    "                start_date=start_date,\n",
    "                end_date=end_date,\n",
    "                batch_size=batch_size,\n",
    "                delay_between_batches=1.0,\n",
    "            )\n",
    "\n",
    "            successful_data.update(batch_data)\n",
    "            pbar.update(len(batch))\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Batch failed: {e}\")\n",
    "            failed_stocks.extend(batch)\n",
    "            pbar.update(len(batch))\n",
    "\n",
    "        # Update progress stats\n",
    "        progress_stats[\"completed\"] = len(successful_data)\n",
    "        progress_stats[\"success_rate\"] = (\n",
    "            len(successful_data) / progress_stats[\"total_stocks\"] * 100\n",
    "        )\n",
    "\n",
    "        # Update progress bar with stats\n",
    "        elapsed_time = time.time() - progress_stats[\"start_time\"]\n",
    "        stocks_per_second = (\n",
    "            progress_stats[\"completed\"] / elapsed_time if elapsed_time > 0 else 0\n",
    "        )\n",
    "\n",
    "        pbar.set_postfix(\n",
    "            {\n",
    "                \"Success\": f\"{progress_stats['success_rate']:.1f}%\",\n",
    "                \"Rate\": f\"{stocks_per_second:.2f}/s\",\n",
    "            }\n",
    "        )\n",
    "\n",
    "    pbar.close()\n",
    "\n",
    "    # Final statistics\n",
    "    total_time = time.time() - progress_stats[\"start_time\"]\n",
    "    print(f\"\\n📊 COMPREHENSIVE FETCH COMPLETED!\")\n",
    "    print(f\"✅ Successfully fetched: {len(successful_data)} stocks\")\n",
    "    print(f\"❌ Failed: {len(failed_stocks)} stocks\")\n",
    "    print(f\"📈 Success rate: {len(successful_data)/len(stock_list)*100:.1f}%\")\n",
    "    print(f\"⏱️  Total time: {total_time/60:.1f} minutes\")\n",
    "    print(f\"🚀 Average rate: {len(successful_data)/total_time:.2f} stocks/second\")\n",
    "\n",
    "    return successful_data, failed_stocks\n",
    "\n",
    "\n",
    "# Execute the comprehensive fetch (limited subset for demo)\n",
    "print(f\"🎯 Starting comprehensive fetch of {len(DEMO_SUBSET)} stocks...\")\n",
    "print(\n",
    "    \"⚠️  This is a DEMO with limited stocks. For ALL stocks, increase the subset size.\"\n",
    ")\n",
    "\n",
    "comprehensive_data, comprehensive_failures = fetch_all_hk_with_progress(\n",
    "    stock_list=DEMO_SUBSET[:10],  # Limit to 10 for demo\n",
    "    start_date=start_date,\n",
    "    end_date=end_date,\n",
    "    batch_size=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Step 4: Production-ready ALL stocks fetching template\n",
    "print(\"🏭 PRODUCTION TEMPLATE: Fetch ALL Hong Kong Stocks\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "\n",
    "def fetch_entire_hk_market(\n",
    "    start_date,\n",
    "    end_date,\n",
    "    checkpoint_every=100,\n",
    "    resume_from_checkpoint=False,\n",
    "    max_workers=1,\n",
    "):\n",
    "    \"\"\"\n",
    "    Production-ready function to fetch ALL HK stocks with:\n",
    "    - Checkpointing for resume capability\n",
    "    - Memory management\n",
    "    - Comprehensive logging\n",
    "    - Error recovery\n",
    "    \"\"\"\n",
    "\n",
    "    # Get complete stock list\n",
    "    all_stocks = get_comprehensive_hk_stock_list()\n",
    "\n",
    "    print(f\"🎯 TARGET: {len(all_stocks)} total HK stocks\")\n",
    "    print(f\"📅 Period: {start_date} to {end_date}\")\n",
    "    print(f\"🔄 Checkpoint every: {checkpoint_every} stocks\")\n",
    "\n",
    "    # Checkpoint file management\n",
    "    checkpoint_file = f\"checkpoint_hk_stocks_{start_date}_{end_date}.json\"\n",
    "    completed_stocks = set()\n",
    "\n",
    "    if resume_from_checkpoint and os.path.exists(checkpoint_file):\n",
    "        with open(checkpoint_file, \"r\") as f:\n",
    "            checkpoint_data = json.load(f)\n",
    "            completed_stocks = set(checkpoint_data.get(\"completed\", []))\n",
    "        print(f\"📋 Resuming from checkpoint: {len(completed_stocks)} already completed\")\n",
    "\n",
    "    # Filter remaining stocks\n",
    "    remaining_stocks = [s for s in all_stocks if s not in completed_stocks]\n",
    "    print(f\"📊 Remaining to fetch: {len(remaining_stocks)} stocks\")\n",
    "\n",
    "    # Estimate resources\n",
    "    estimated_hours = len(remaining_stocks) * 1.5 / 3600\n",
    "    estimated_gb = len(remaining_stocks) * 0.001  # ~1MB per stock\n",
    "\n",
    "    print(f\"⏱️  Estimated time: {estimated_hours:.1f} hours\")\n",
    "    print(f\"💾 Estimated storage: {estimated_gb:.2f} GB\")\n",
    "    print(f\"🌐 Estimated API calls: {len(remaining_stocks):,}\")\n",
    "\n",
    "    # WARNING and confirmation\n",
    "    print(f\"\\n🚨 WARNING: This will make {len(remaining_stocks):,} API calls!\")\n",
    "    print(\"🚨 This operation will take several hours to complete.\")\n",
    "    print(\"🚨 Ensure you have sufficient API quota and storage space.\")\n",
    "\n",
    "    # For safety, we'll just show the template without executing\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"📝 TEMPLATE READY - Execute with caution!\")\n",
    "    print(\"💡 TIP: Start with a smaller subset to test your setup first.\")\n",
    "    print(\"💡 TIP: Run during off-peak hours to avoid rate limiting.\")\n",
    "    print(\"💡 TIP: Monitor your API usage regularly.\")\n",
    "\n",
    "    return {\n",
    "        \"total_stocks\": len(all_stocks),\n",
    "        \"remaining_stocks\": len(remaining_stocks),\n",
    "        \"completed_stocks\": len(completed_stocks),\n",
    "        \"estimated_hours\": estimated_hours,\n",
    "        \"template_ready\": True,\n",
    "    }\n",
    "\n",
    "\n",
    "# Show the production template (without executing)\n",
    "production_info = fetch_entire_hk_market(start_date=start_date, end_date=end_date)\n",
    "\n",
    "print(f\"\\n📊 Production Analysis Complete:\")\n",
    "for key, value in production_info.items():\n",
    "    print(f\"   {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Import json for checkpoint functionality\n",
    "import json\n",
    "\n",
    "# Step 5: Execute Full Market Fetch (UNCOMMENT TO RUN)\n",
    "print(\"⚠️  FULL MARKET EXECUTION CODE (Currently Commented for Safety)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# UNCOMMENT AND MODIFY THESE LINES TO EXECUTE FULL MARKET FETCH:\n",
    "\n",
    "execute_full_fetch = True  # Set to True to execute\n",
    "\n",
    "if execute_full_fetch:\n",
    "    print(\"🚀 EXECUTING FULL HK MARKET FETCH...\")\n",
    "\n",
    "    # WARNING: This will take hours and use significant API quota\n",
    "    full_market_data = fetch_all_hk_with_progress(\n",
    "        stock_list=all_hk_stocks,  # ALL stocks\n",
    "        start_date=start_date,\n",
    "        end_date=end_date,\n",
    "        batch_size=10,\n",
    "    )\n",
    "\n",
    "    # Save the complete dataset\n",
    "    save_bulk_data(\n",
    "        stock_data_dict=full_market_data[0],\n",
    "        output_dir=\"data/full_hk_market\",\n",
    "        file_format=\"csv\",\n",
    "    )\n",
    "\n",
    "    print(\"✅ FULL HK MARKET FETCH COMPLETED!\")\n",
    "\n",
    "else:\n",
    "    print(\"🛡️  Full market fetch is DISABLED for safety.\")\n",
    "    print(\"📋 To execute the full market fetch:\")\n",
    "    print(\"   1. Set execute_full_fetch = True\")\n",
    "    print(\"   2. Ensure you have sufficient API quota\")\n",
    "    print(\"   3. Prepare for several hours of execution time\")\n",
    "    print(\"   4. Monitor system resources and API usage\")\n",
    "    print(\"   5. Consider running during off-peak hours\")\n",
    "\n",
    "    print(\n",
    "        f\"\\n📊 READY TO FETCH: {len(all_hk_stocks) if 'all_hk_stocks' in locals() else 'N/A'} total HK stocks\"\n",
    "    )\n",
    "    print(\"🚀 All infrastructure is in place for full market analysis!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Save individual stock data to CSV files\n",
    "print(\"💾 Saving bulk data to files...\")\n",
    "\n",
    "if demo_stocks:\n",
    "    saved_files = save_bulk_data(\n",
    "        stock_data_dict=demo_stocks, output_dir=\"data\", file_format=\"csv\"\n",
    "    )\n",
    "    print(f\"✅ Saved {len(saved_files)} stock data files\")\n",
    "    print(f\"📁 Files saved to: {saved_files[:3]}...\")  # Show first 3 files\n",
    "\n",
    "# Create a comprehensive summary\n",
    "if demo_stocks:\n",
    "    summary_df = create_bulk_fetch_summary(demo_stocks)\n",
    "    print(\"\\n📊 Comprehensive Stock Summary:\")\n",
    "    print(\"=\" * 60)\n",
    "    display(summary_df)\n",
    "\n",
    "    # Save summary to CSV\n",
    "    summary_file = \"data/bulk_stock_summary.csv\"\n",
    "    summary_df.to_csv(summary_file, index=False)\n",
    "    print(f\"\\n💾 Summary saved to: {summary_file}\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "# Example of parallel processing (use sparingly)\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "\n",
    "def fetch_single_stock_with_delay(stock_symbol):\n",
    "    \"\"\"Fetch a single stock with built-in delay\"\"\"\n",
    "    try:\n",
    "        time.sleep(0.5)  # Conservative delay\n",
    "        # This would call your actual fetching function\n",
    "        print(f\"🔄 Fetching {stock_symbol}...\")\n",
    "        return f\"Data for {stock_symbol}\"\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error fetching {stock_symbol}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# Demonstrate parallel processing with a small subset\n",
    "small_stock_list = all_major_stocks[:5]  # Only use 5 stocks for demo\n",
    "\n",
    "print(\"🚀 Parallel Processing Demo (5 stocks):\")\n",
    "print(\"⚠️  Using conservative delays to respect rate limits\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Use ThreadPoolExecutor with limited workers\n",
    "with ThreadPoolExecutor(max_workers=2) as executor:  # Only 2 concurrent requests\n",
    "    results = list(executor.map(fetch_single_stock_with_delay, small_stock_list))\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"\\n✅ Parallel processing completed in {end_time - start_time:.2f} seconds\")\n",
    "print(f\"📊 Successfully processed {len([r for r in results if r])} stocks\")\n",
    "print(\n",
    "    \"\\n⚠️  **Note**: Always test rate limits before using parallel processing in production!\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Method 6: Error Handling & Recovery\n",
    "\n",
    "Robust error handling is crucial when fetching large amounts of data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Example of robust error handling during bulk fetching\n",
    "\n",
    "\n",
    "def robust_bulk_fetch(stock_list, start_date, end_date, max_retries=3):\n",
    "    \"\"\"\n",
    "    Fetch stocks with retry logic and error handling\n",
    "    \"\"\"\n",
    "    successful_fetches = {}\n",
    "    failed_stocks = []\n",
    "\n",
    "    for stock in stock_list:\n",
    "        retry_count = 0\n",
    "        success = False\n",
    "\n",
    "        while retry_count < max_retries and not success:\n",
    "            try:\n",
    "                print(f\"🔄 Fetching {stock} (attempt {retry_count + 1}/{max_retries})\")\n",
    "\n",
    "                # Simulate API call with potential failure\n",
    "                import random\n",
    "\n",
    "                if random.random() < 0.8:  # 80% success rate for demo\n",
    "                    # This would be your actual data fetching call\n",
    "                    successful_fetches[stock] = f\"Mock data for {stock}\"\n",
    "                    success = True\n",
    "                    print(f\"✅ Successfully fetched {stock}\")\n",
    "                else:\n",
    "                    raise Exception(\"Simulated API error\")\n",
    "\n",
    "            except Exception as e:\n",
    "                retry_count += 1\n",
    "                print(f\"⚠️  Error fetching {stock}: {e}\")\n",
    "\n",
    "                if retry_count < max_retries:\n",
    "                    wait_time = retry_count * 2  # Exponential backoff\n",
    "                    print(f\"⏱️  Waiting {wait_time} seconds before retry...\")\n",
    "                    time.sleep(wait_time)\n",
    "                else:\n",
    "                    failed_stocks.append(stock)\n",
    "                    print(f\"❌ Failed to fetch {stock} after {max_retries} attempts\")\n",
    "\n",
    "    return successful_fetches, failed_stocks\n",
    "\n",
    "\n",
    "# Demo with a small subset\n",
    "demo_stock_subset = all_major_stocks[:5]\n",
    "print(f\"🚀 Testing robust fetching with {len(demo_stock_subset)} stocks:\")\n",
    "\n",
    "successful_data, failed_list = robust_bulk_fetch(\n",
    "    stock_list=demo_stock_subset,\n",
    "    start_date=start_date,\n",
    "    end_date=end_date,\n",
    "    max_retries=2,\n",
    ")\n",
    "\n",
    "print(f\"\\n📊 Results:\")\n",
    "print(f\"✅ Successful: {len(successful_data)} stocks\")\n",
    "print(f\"❌ Failed: {len(failed_list)} stocks\")\n",
    "if failed_list:\n",
    "    print(f\"📝 Failed stocks: {failed_list}\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 📊 Summary & Best Practices\n",
    "\n",
    "### 🎯 Key Takeaways\n",
    "\n",
    "1. **Start Small**: Begin with curated lists of major stocks (most reliable)\n",
    "2. **Respect Rate Limits**: Use delays between requests (0.5-2 seconds recommended)\n",
    "3. **Batch Processing**: Process stocks in small batches (5-10 stocks per batch)\n",
    "4. **Error Handling**: Implement retry logic with exponential backoff\n",
    "5. **Data Management**: Organize and save data systematically\n",
    "6. **Monitoring**: Track progress and success rates\n",
    "\n",
    "### ⚡ Performance Tips\n",
    "\n",
    "- **Sequential Processing**: Safest approach, respects API limits\n",
    "- **Parallel Processing**: Use sparingly with conservative limits (max 2-3 workers)\n",
    "- **Caching**: Save fetched data to avoid re-fetching\n",
    "- **Incremental Updates**: Only fetch new data when needed\n",
    "\n",
    "### 🛡️ Risk Management\n",
    "\n",
    "- Always test with small datasets first\n",
    "- Monitor API response times and error rates\n",
    "- Have fallback strategies for failed requests\n",
    "- Keep track of API usage quotas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Final demonstration: Choose your approach based on needs\n",
    "\n",
    "print(\"🎯 RECOMMENDED APPROACHES:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"\\n1. 🔰 **BEGINNER**: Start with top 10-20 stocks\")\n",
    "print(\"   - Use: fetch_all_major_hk_stocks(max_stocks=20)\")\n",
    "print(\"   - Safe, reliable, fast\")\n",
    "\n",
    "print(\"\\n2. 📊 **SECTOR ANALYSIS**: Focus on specific sectors\")\n",
    "print(\"   - Use: fetch_hk_tech_stocks() or get_hk_stocks_by_sector()\")\n",
    "print(\"   - Targeted analysis, manageable size\")\n",
    "\n",
    "print(\"\\n3. 🚀 **ADVANCED**: Full market analysis\")\n",
    "print(\"   - Use: fetch_all_major_hk_stocks() with careful batching\")\n",
    "print(\"   - Comprehensive but requires patience\")\n",
    "\n",
    "print(\"\\n4. ⚡ **ENTERPRISE**: High-volume processing\")\n",
    "print(\"   - Implement parallel processing with rate limiting\")\n",
    "print(\"   - Requires careful monitoring and error handling\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"✅ Notebook completed successfully!\")\n",
    "print(\"📚 You now have tools for any scale of HK stock data collection!\")\n",
    "print(\"🚀 Happy analyzing! 🇭🇰📈\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "ipynb,py:percent",
   "notebook_metadata_filter": "all,-language_info,-toc,-latex_envs"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
