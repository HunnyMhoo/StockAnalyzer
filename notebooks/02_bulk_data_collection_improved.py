# ---
# jupyter:
#   jupytext:
#     cell_metadata_filter: -all
#     formats: ipynb,py:percent
#     notebook_metadata_filter: all,-language_info,-toc,-latex_envs
#     text_representation:
#       extension: .py
#       format_name: percent
#       format_version: '1.3'
#       jupytext_version: 1.17.2
#   kernelspec:
#     display_name: Python 3
#     language: python
#     name: python3
# ---

# %% [raw]
# # Hong Kong Stock Bulk Data Collection - Comprehensive Guide
#
# This notebook provides **efficient, production-ready approaches** for bulk fetching Hong Kong stock data.
#
# ## ğŸ“‹ **What You'll Master**
#
# | Level | Approach | Best For |
# |-------|----------|----------|
# | ğŸ”° **Beginner** | Curated Stock Lists | Learning & Testing (10-50 stocks) |
# | ğŸ“Š **Intermediate** | Sector-Based Fetching | Targeted Analysis (50-200 stocks) |
# | ğŸš€ **Advanced** | Full Universe Discovery | Comprehensive Research (500+ stocks) |
# | âš¡ **Enterprise** | Parallel Processing | Production Systems (1000+ stocks) |
#
# ## ğŸ¯ **Key Features**
# - **Smart Rate Limiting**: Respects API constraints
# - **Error Recovery**: Robust retry logic with exponential backoff
# - **Progress Tracking**: Real-time monitoring for large operations
# - **Data Management**: Systematic saving and organization
# - **Memory Efficient**: Batch processing to handle large datasets
#
# ---
# **ğŸ“ Note**: Run the Setup section first, then choose your preferred approach.
#

# %% [raw]
# ## ğŸ”§ **Setup & Configuration**
#
# **Run this section first** - All subsequent cells depend on this setup.
#

# %%
# Standard Library Imports
import sys
import os
from pathlib import Path
import pandas as pd
from datetime import datetime, timedelta
import warnings
warnings.filterwarnings('ignore')

# Setup Python Path
notebook_dir = Path().absolute()
src_dir = notebook_dir.parent / 'src'
sys.path.insert(0, str(src_dir))

print("âœ… Standard libraries loaded")
print(f"ğŸ“ Source path: {src_dir}")
print(f"ğŸ“ Working directory: {notebook_dir}")

# Verify setup
if src_dir.exists():
    print("ğŸ¯ Setup successful - ready to proceed!")
else:
    print("âš ï¸ Source directory not found - check your notebook location")


# %%
# Import Custom Modules and Improved Utilities
try:
    # Core stock data modules
    from hk_stock_universe import (
        get_hk_stock_list_static,
        get_hk_stocks_by_sector,
        get_comprehensive_hk_stock_list,
        MAJOR_HK_STOCKS
    )
    
    from bulk_data_fetcher import (
        fetch_hk_stocks_bulk,
        fetch_all_major_hk_stocks,
        fetch_hk_tech_stocks,
        create_bulk_fetch_summary,
        save_bulk_data
    )
    
    # Import the improved utilities we created
    from bulk_collection_improved import (
        BulkCollectionConfig,
        BulkCollector,
        ResultsManager,
        create_beginner_collector,
        create_enterprise_collector,
        quick_demo
    )
    
    print("âœ… All modules imported successfully!")
    print("ğŸš€ Enhanced utilities loaded - ready for efficient bulk collection!")
    
except ImportError as e:
    print(f"âŒ Import error: {e}")
    print("\nğŸ”§ Troubleshooting:")
    print("   1. Ensure you're running from the notebooks/ directory")
    print("   2. Check that all source files exist in ../src/")
    print("   3. Verify Python path setup in previous cell")
    raise


# %%
# Global Configuration - Centralized Settings
config = BulkCollectionConfig()

print("ğŸ“‹ **Global Configuration**")
print("=" * 40)
print(f"ğŸ“… Date Range: {config.start_date} â†’ {config.end_date}")
print(f"ğŸ“Š Period: {config.default_period_days} days")
print(f"âš¡ Batch Sizes: {config.batch_sizes}")
print(f"â±ï¸ Delays: {config.delays}")
print(f"ğŸ”„ Max Retries: {config.max_retries}")
print(f"ğŸ›¡ï¸ Parallel Workers: {config.max_workers}")

print("\nğŸ¯ **Configuration Ready!**")
print("Choose your approach below based on your needs.")


# %% [raw]
# ---
# # ğŸ¯ **Choose Your Approach**
#
# Select the section that matches your experience level and requirements:
#
# | ğŸ”° **Beginner** | ğŸ“Š **Intermediate** | ğŸš€ **Advanced** | âš¡ **Enterprise** |
# |-----------------|---------------------|------------------|-------------------|
# | 10-50 stocks | 50-200 stocks | 200-500 stocks | 500+ stocks |
# | Safe & Simple | Sector Analysis | Full Universe | Parallel Processing |
# | 2-5 minutes | 10-30 minutes | 30-120 minutes | 2+ hours |
#

# %% [raw]
# ## ğŸ”° **Level 1: Beginner - Quick Start**
#
# **Perfect for**: Learning, testing, small analysis projects
# **Time Required**: 2-5 minutes
# **Stocks**: 10-50 stocks
#
# ### Features:
# - âœ… Conservative rate limiting (safe for any API)
# - âœ… Small batch sizes (easy to manage)
# - âœ… Clear progress tracking
# - âœ… Automatic error handling
#

# %%
# Beginner Approach: Quick & Safe Stock Universe Exploration
print("ğŸ”° **BEGINNER LEVEL: Stock Universe Overview**")
print("=" * 60)

# Explore available stock categories (no API calls)
print("\nğŸ“Š **Available HK Stock Sectors:**")
sector_summary = []
for sector, stocks in MAJOR_HK_STOCKS.items():
    sector_summary.append({
        'Sector': sector.replace('_', ' ').title(),
        'Count': len(stocks),
        'Examples': ', '.join(stocks[:3])
    })

sector_df = pd.DataFrame(sector_summary)
display(sector_df)

# Get major stocks overview
all_major_stocks = get_hk_stock_list_static()
print(f"\nğŸ“ˆ **Total Major Stocks Available**: {len(all_major_stocks)}")
print(f"ğŸ” **Sample Tickers**: {all_major_stocks[:10]}")

# Select specific sectors for analysis
tech_stocks = get_hk_stocks_by_sector('tech_stocks')
finance_stocks = get_hk_stocks_by_sector('finance')

print(f"\nğŸ’¡ **Sector Breakdown:**")
print(f"   ğŸ’» Tech Sector: {len(tech_stocks)} stocks")
print(f"   ğŸ¦ Finance Sector: {len(finance_stocks)} stocks")
print(f"   ğŸ“Š Total Unique: {len(set(tech_stocks + finance_stocks))} stocks")

print("\nâœ… **Stock Universe Exploration Complete!**")
print("ğŸ“‹ Ready to fetch data using the improved utilities below.")


# %%
# Beginner Demo: Fetch Top 10 Stocks with Enhanced Utilities
print("ğŸš€ **BEGINNER DEMO: Fetching Top 10 HK Stocks**")
print("Using improved utilities for better experience!")

# Create beginner-optimized collector
beginner_collector = create_beginner_collector()

print(f"\nğŸ“‹ **Beginner Configuration:**")
print(f"   â±ï¸ Delay: {beginner_collector.config.get_delay('normal')}s (conservative)")
print(f"   ğŸ“¦ Batch Size: {beginner_collector.config.get_batch_size('medium')} stocks")
print(f"   ğŸ”„ Max Retries: {beginner_collector.config.max_retries}")

# Select first 10 major stocks for demo
demo_stocks = all_major_stocks[:10]
print(f"\nğŸ¯ **Target Stocks**: {demo_stocks}")

# Fetch data using improved collector
print(f"\nğŸ”„ **Starting Fetch Process...**")
results = beginner_collector.fetch_sequential(
    stock_list=demo_stocks,
    fetch_function=fetch_hk_stocks_bulk,
    level='small'
)

# Display results using improved manager
print(f"\nğŸ“Š **RESULTS SUMMARY**")
ResultsManager.display_summary(results['data'], "Beginner Demo Results")

# Show performance statistics
stats = results['statistics']
print(f"\nâš¡ **Performance Metrics:**")
print(f"   â±ï¸ Total Time: {stats['total_time']:.1f}s")
print(f"   ğŸ“ˆ Success Rate: {stats['success_rate']:.1%}")
print(f"   ğŸš€ Processing Rate: {stats['processing_rate']:.2f} stocks/sec")
print(f"   âœ… Successful: {stats['successful']} stocks")
print(f"   âŒ Failed: {stats['failed']} stocks")

# Store results for potential use in other cells
beginner_results = results
print(f"\nğŸ‰ **Beginner Demo Complete!** Results stored in 'beginner_results'.")


# %% [raw]
# ## ğŸ“Š **Level 2: Intermediate - Sector Analysis**
#
# **Perfect for**: Targeted analysis, sector research, comparative studies
# **Time Required**: 10-30 minutes  
# **Stocks**: 50-200 stocks
#
# ### Features:
# - âœ… Sector-specific data collection
# - âœ… Optimized batch processing
# - âœ… Comprehensive progress tracking
# - âœ… Advanced error recovery
#

# %%
# Intermediate: Sector-Focused Analysis
print("ğŸ“Š **INTERMEDIATE LEVEL: Sector-Based Collection**")
print("=" * 60)

# Create standard collector for intermediate use
intermediate_collector = BulkCollector(config)

# Select sectors for analysis (you can modify this)
selected_sectors = ['tech_stocks', 'finance']  # Modify as needed
print(f"ğŸ¯ **Selected Sectors**: {[s.replace('_', ' ').title() for s in selected_sectors]}")

# Collect data for each sector
sector_results = {}
total_stocks_processed = 0

for sector in selected_sectors:
    print(f"\nğŸ”„ **Processing {sector.replace('_', ' ').title()} Sector**")
    
    if sector == 'tech_stocks':
        # Use specialized tech fetcher
        print("   ğŸ’» Using specialized tech stock fetcher...")
        sector_data = fetch_hk_tech_stocks(
            start_date=config.start_date,
            end_date=config.end_date,
            batch_size=config.get_batch_size('medium'),
            delay_between_batches=config.get_delay('normal')
        )
        # Convert to our result format
        sector_result = {
            'data': sector_data,
            'statistics': {
                'successful': len(sector_data),
                'failed': 0,
                'total_time': 0,  # Would be calculated in real fetch
                'success_rate': 1.0 if sector_data else 0.0
            }
        }
    else:
        # Use generic bulk fetcher for other sectors
        sector_stocks = get_hk_stocks_by_sector(sector)
        print(f"   ğŸ“Š Fetching {len(sector_stocks)} stocks...")
        
        sector_result = intermediate_collector.fetch_sequential(
            stock_list=sector_stocks,
            fetch_function=fetch_hk_stocks_bulk,
            level='medium'
        )
    
    sector_results[sector] = sector_result
    stocks_fetched = len(sector_result['data'])
    total_stocks_processed += stocks_fetched
    
    print(f"   âœ… {sector.replace('_', ' ').title()}: {stocks_fetched} stocks fetched")
    
    # Display sector summary
    ResultsManager.display_summary(
        sector_result['data'], 
        f"{sector.replace('_', ' ').title()} Sector Results"
    )

# Overall summary
print(f"\nğŸ‰ **INTERMEDIATE ANALYSIS COMPLETE**")
print(f"ğŸ“Š **Total Stocks Processed**: {total_stocks_processed}")
print(f"ğŸ¢ **Sectors Analyzed**: {len(selected_sectors)}")

# Show comparative statistics
print(f"\nğŸ“ˆ **Sector Comparison:**")
for sector, result in sector_results.items():
    stats = result['statistics']
    print(f"   {sector.replace('_', ' ').title()}: {stats['successful']} stocks "
          f"({stats['success_rate']:.1%} success rate)")

# Store results for potential use
intermediate_results = sector_results
print(f"\nğŸ’¾ Results stored in 'intermediate_results' for further analysis.")


# %% [raw]
# ## ğŸš€ **Level 3: Advanced - Full Universe Discovery**
#
# **Perfect for**: Comprehensive research, market analysis, data science projects
# **Time Required**: 30-120 minutes
# **Stocks**: 200-500+ stocks
#
# ### Features:
# - âœ… Complete HK stock universe discovery
# - âœ… Systematic sampling strategies
# - âœ… Checkpoint support for resume capability
# - âœ… Memory-efficient processing
# - âœ… Comprehensive error handling
#

# %%
# Advanced: Full Universe Discovery and Analysis
print("ğŸš€ **ADVANCED LEVEL: Complete HK Universe Discovery**")
print("=" * 70)

# Discover complete HK stock universe
print("ğŸ” **Discovering Complete HK Stock Universe...**")
print("âš ï¸ This may take a few minutes for validation...")

universe_config = {
    'include_major': True,
    'validate_tickers': True,
    'max_tickers': 200  # Adjust: None for complete universe, 200 for demo
}

stock_universe = get_comprehensive_hk_stock_list(**universe_config)
all_discovered_stocks = sorted(stock_universe['valid_stocks'])

print(f"\nğŸ“Š **Universe Discovery Results:**")
print(f"   âœ… Valid Stocks: {len(stock_universe['valid_stocks'])}")
print(f"   âŒ Invalid Stocks: {len(stock_universe['invalid_stocks'])}")
print(f"   ğŸ“ˆ Range: {all_discovered_stocks[0]} â†’ {all_discovered_stocks[-1]}")

# Resource estimation for full universe
total_stocks = len(all_discovered_stocks)
estimated_time_hours = total_stocks * 1.5 / 3600
estimated_api_calls = total_stocks
estimated_data_mb = total_stocks * 0.1

print(f"\nâš–ï¸ **Resource Requirements:**")
print(f"   â±ï¸ Estimated Time: {estimated_time_hours:.1f} hours")
print(f"   ğŸŒ API Calls: {estimated_api_calls:,}")
print(f"   ğŸ’¾ Data Size: ~{estimated_data_mb:.1f} MB")

# Smart sampling for demonstration
DEMO_SIZE = 50  # Adjust based on your needs
if total_stocks > DEMO_SIZE:
    print(f"\nğŸ“‹ **Demo Mode**: Using systematic sample of {DEMO_SIZE} stocks")
    # Systematic sampling for variety
    step = max(1, total_stocks // DEMO_SIZE)
    demo_stocks = all_discovered_stocks[::step][:DEMO_SIZE]
    print(f"   ğŸ“ Sample: Every {step}th stock")
else:
    demo_stocks = all_discovered_stocks
    print(f"\nğŸ“‹ **Complete Set**: Using all {total_stocks} discovered stocks")

print(f"ğŸ¯ **Target for Advanced Demo**: {len(demo_stocks)} stocks")
print(f"ğŸ“Š **Sample Range**: {demo_stocks[0]} â†’ {demo_stocks[-1]}")

# Store universe data for next cell
advanced_stock_universe = demo_stocks
print(f"\nâœ… **Universe Discovery Complete!** Ready for advanced fetching.")


# %%
# Advanced: Execute with Checkpointing Support
print("ğŸš§ **ADVANCED EXECUTION: Fetch with Checkpoint Support**")
print("=" * 60)

# Create advanced collector
advanced_collector = BulkCollector(config)

# Safety check and user confirmation
EXECUTE_ADVANCED = True  # Set to True to execute
CHECKPOINT_DIR = "data/advanced_checkpoints"

print(f"âš™ï¸ **Advanced Configuration:**")
print(f"   ğŸ“¦ Batch Size: {config.get_batch_size('large')}")
print(f"   â±ï¸ Delay: {config.get_delay('conservative')}s")
print(f"   ğŸ’¾ Checkpoint Dir: {CHECKPOINT_DIR}")
print(f"   ğŸ¯ Target Stocks: {len(advanced_stock_universe)}")

if not EXECUTE_ADVANCED:
    print("\nğŸ›¡ï¸ **Safety Mode**: Advanced execution disabled")
    print("ğŸ’¡ Set EXECUTE_ADVANCED = True to run comprehensive fetch")
    print(f"ğŸ“Š Ready to process {len(advanced_stock_universe)} stocks")
    
else:
    print("\nğŸš€ **EXECUTING ADVANCED FETCH WITH CHECKPOINTS**")
    
    # Option 1: Checkpoint-enabled fetch (recommended for large operations)
    print("ğŸ”„ **Method: Checkpoint-Enabled Fetch**")
    print("ğŸ’¡ Can be interrupted and resumed at any time")
    
    advanced_results = advanced_collector.fetch_with_checkpoints(
        stock_list=advanced_stock_universe,
        fetch_function=fetch_hk_stocks_bulk,
        checkpoint_dir=CHECKPOINT_DIR,
        checkpoint_every=25  # Save progress every 25 stocks
    )
    
    # Display comprehensive results
    if advanced_results['data']:
        print(f"\nğŸ“Š **ADVANCED RESULTS SUMMARY**")
        ResultsManager.display_summary(
            advanced_results['data'], 
            "Advanced Universe Discovery Results"
        )
        
        print(f"\nğŸ‰ **Advanced Analysis Complete!**")
        print(f"âœ… Successfully fetched: {len(advanced_results['data'])} stocks")
        print(f"âŒ Failed: {len(advanced_results['failed'])} stocks")
        
        # Save comprehensive dataset
        print(f"\nğŸ’¾ **Saving Advanced Dataset...**")
        try:
            save_result = ResultsManager.save_with_metadata(
                data_dict=advanced_results['data'],
                metadata={
                    'collection_type': 'advanced_universe',
                    'total_stocks': len(advanced_stock_universe),
                    'successful': len(advanced_results['data']),
                    'failed': len(advanced_results['failed']),
                    'timestamp': datetime.now().isoformat(),
                    'config': config.__dict__
                },
                output_dir="data/advanced_universe"
            )
            print(f"âœ… Dataset saved: {save_result['total_files']} files")
            print(f"ğŸ“„ Metadata: {save_result['metadata_file']}")
            
        except Exception as e:
            print(f"âš ï¸ Save operation failed: {e}")
    
    else:
        print("âŒ No data collected - check error logs")

# Store results
if 'advanced_results' in locals():
    print(f"\nğŸ’¾ Results stored in 'advanced_results' variable")
else:
    print(f"\nğŸ’¡ Execute the cell with EXECUTE_ADVANCED = True to run the fetch")


# %% [raw]
# ## âš¡ **Level 4: Enterprise - Parallel Processing**
#
# **Perfect for**: Production systems, time-critical analysis, large-scale operations
# **Time Required**: 2+ hours (depending on scale)
# **Stocks**: 500+ stocks
#
# ### Features:
# - âœ… Safe parallel processing with rate limiting
# - âœ… Enterprise-grade error handling
# - âœ… Performance monitoring and optimization
# - âœ… Production deployment templates
#
# ### âš ï¸ **WARNING**: Use with extreme caution - can overwhelm APIs!
#

# %%
# Enterprise: Parallel Processing Demo with Safety Controls
print("âš¡ **ENTERPRISE LEVEL: Parallel Processing with Safety**")
print("=" * 70)

# Enterprise configuration
class EnterpriseConfig:
    """Enterprise-specific settings with safety controls"""
    MAX_WORKERS = 2  # Conservative - increase after testing
    WORKER_DELAY = 1.0  # Delay per worker request
    ENABLE_PARALLEL = False  # Safety switch - must be explicitly enabled
    DEMO_SIZE = 10  # Small demo for safety

print(f"ğŸ›¡ï¸ **Enterprise Safety Configuration:**")
print(f"   âš¡ Max Workers: {EnterpriseConfig.MAX_WORKERS}")
print(f"   â±ï¸ Worker Delay: {EnterpriseConfig.WORKER_DELAY}s")
print(f"   ğŸ”’ Parallel Enabled: {EnterpriseConfig.ENABLE_PARALLEL}")
print(f"   ğŸ“Š Demo Size: {EnterpriseConfig.DEMO_SIZE}")

# Create enterprise collector
enterprise_collector = create_enterprise_collector()

# Demo dataset for parallel processing
if 'all_major_stocks' in locals():
    demo_parallel_stocks = all_major_stocks[:EnterpriseConfig.DEMO_SIZE]
else:
    # Fallback demo stocks
    demo_parallel_stocks = ['0700.HK', '0005.HK', '0941.HK', '1299.HK', '2318.HK']

print(f"\nğŸ¯ **Parallel Demo Target**: {demo_parallel_stocks}")

if EnterpriseConfig.ENABLE_PARALLEL:
    print(f"\nâš¡ **EXECUTING PARALLEL PROCESSING**")
    print(f"ğŸš¨ WARNING: Using {EnterpriseConfig.MAX_WORKERS} workers with {EnterpriseConfig.WORKER_DELAY}s delays")
    
    # Execute parallel processing
    parallel_results = enterprise_collector.fetch_parallel(
        stock_list=demo_parallel_stocks,
        fetch_function=fetch_hk_stocks_bulk,
        max_workers=EnterpriseConfig.MAX_WORKERS
    )
    
    # Display results
    print(f"\nğŸ“Š **PARALLEL PROCESSING RESULTS**")
    ResultsManager.display_summary(
        parallel_results['data'],
        "Enterprise Parallel Processing Results"
    )
    
    # Performance analysis
    stats = parallel_results['statistics']
    print(f"\nâš¡ **Performance Analysis:**")
    print(f"   â±ï¸ Total Time: {stats['total_time']:.1f}s")
    print(f"   ğŸ“ˆ Success Rate: {stats['success_rate']:.1%}")
    print(f"   ğŸš€ Processing Rate: {stats['processing_rate']:.2f} stocks/sec")
    print(f"   ğŸ‘¥ Workers Used: {parallel_results['metadata']['max_workers']}")
    
    # Compare with sequential baseline
    print(f"\nğŸ“Š **Sequential vs Parallel Comparison:**")
    print(f"   Sequential (estimated): {len(demo_parallel_stocks) * EnterpriseConfig.WORKER_DELAY:.1f}s")
    print(f"   Parallel (actual): {stats['total_time']:.1f}s")
    speedup = (len(demo_parallel_stocks) * EnterpriseConfig.WORKER_DELAY) / stats['total_time']
    print(f"   ğŸš€ Speedup: {speedup:.1f}x faster")
    
    enterprise_results = parallel_results
    
else:
    print(f"\nğŸ›¡ï¸ **PARALLEL PROCESSING DISABLED FOR SAFETY**")
    print(f"ğŸ”§ **To Enable Enterprise Parallel Processing:**")
    print(f"   1. Set EnterpriseConfig.ENABLE_PARALLEL = True")
    print(f"   2. Test with small datasets first (5-10 stocks)")
    print(f"   3. Monitor API response times carefully")
    print(f"   4. Gradually increase workers (2 â†’ 4 â†’ 8)")
    print(f"   5. Implement comprehensive monitoring")
    print(f"   6. Have fallback to sequential processing")
    
    print(f"\nâš¡ **Enterprise Infrastructure Ready:**")
    print(f"   ğŸ“Š Demo Stocks: {len(demo_parallel_stocks)}")
    print(f"   ğŸ”§ Workers Available: {EnterpriseConfig.MAX_WORKERS}")
    estimated_time = len(demo_parallel_stocks) * EnterpriseConfig.WORKER_DELAY / EnterpriseConfig.MAX_WORKERS
    print(f"   â±ï¸ Estimated Time: {estimated_time:.1f}s")
    print(f"   ğŸ’¡ Ready for production deployment!")

print(f"\nâœ… **Enterprise Level Complete**")


# %% [raw]
# ---
# ## ğŸ“Š **Summary & Best Practices**
#
# ### ğŸ¯ **Quick Reference Guide**
#
# | Your Need | Level | Typical Time | Best Approach |
# |-----------|-------|--------------|---------------|
# | Learning & Testing | ğŸ”° Beginner | 2-5 min | `create_beginner_collector()` |
# | Sector Analysis | ğŸ“Š Intermediate | 10-30 min | `BulkCollector()` with sector lists |
# | Market Research | ğŸš€ Advanced | 30-120 min | `fetch_with_checkpoints()` |
# | Production System | âš¡ Enterprise | 2+ hours | `create_enterprise_collector()` |
#
# ### ğŸ›¡ï¸ **Critical Success Factors**
# 1. **Start Small**: Always test with 5-10 stocks first
# 2. **Rate Limiting**: Respect API constraints (1-2s delays minimum)  
# 3. **Error Handling**: Implement retry logic and checkpointing
# 4. **Monitoring**: Track success rates and performance metrics
# 5. **Progressive Scaling**: Gradually increase batch sizes and workers
#
# ### ğŸ“ˆ **Performance Optimization Tips**
# - Use checkpointing for operations > 100 stocks
# - Monitor memory usage for large datasets
# - Implement caching for frequently accessed data  
# - Use systematic sampling for universe discovery
# - Have fallback strategies for API failures
#

# %%
# Final Summary and Next Steps
print("ğŸ‰ **BULK DATA COLLECTION NOTEBOOK COMPLETE!**")
print("=" * 70)

# Collect all results if available
all_results = {}
result_summary = []

# Check what results we have from each level
if 'beginner_results' in locals():
    all_results['beginner'] = beginner_results
    result_summary.append({
        'Level': 'ğŸ”° Beginner',
        'Stocks': len(beginner_results['data']),
        'Success_Rate': f"{beginner_results['statistics']['success_rate']:.1%}",
        'Time': f"{beginner_results['statistics']['total_time']:.1f}s"
    })

if 'intermediate_results' in locals():
    total_intermediate = sum(len(r['data']) for r in intermediate_results.values())
    all_results['intermediate'] = intermediate_results
    result_summary.append({
        'Level': 'ğŸ“Š Intermediate',
        'Stocks': total_intermediate,
        'Success_Rate': 'Varies by sector',
        'Time': 'Varies by sector'
    })

if 'advanced_results' in locals():
    all_results['advanced'] = advanced_results
    result_summary.append({
        'Level': 'ğŸš€ Advanced',
        'Stocks': len(advanced_results['data']) if 'data' in advanced_results else 0,
        'Success_Rate': 'Checkpointed',
        'Time': 'Variable'
    })

if 'enterprise_results' in locals():
    all_results['enterprise'] = enterprise_results
    result_summary.append({
        'Level': 'âš¡ Enterprise',
        'Stocks': len(enterprise_results['data']),
        'Success_Rate': f"{enterprise_results['statistics']['success_rate']:.1%}",
        'Time': f"{enterprise_results['statistics']['total_time']:.1f}s"
    })

# Display session summary
if result_summary:
    print(f"\nğŸ“Š **SESSION RESULTS SUMMARY**")
    summary_df = pd.DataFrame(result_summary)
    display(summary_df)
    
    total_stocks = sum(int(str(row['Stocks']).split()[0]) if isinstance(row['Stocks'], str) else row['Stocks'] 
                      for row in result_summary)
    print(f"\nğŸ¯ **Total Stocks Processed This Session**: {total_stocks}")
else:
    print(f"\nğŸ’¡ **No results collected** - Run the level sections above to see data collection in action!")

print(f"\nğŸš€ **WHAT YOU'VE ACCOMPLISHED:**")
print(f"âœ… Learned 4 different approaches to bulk data collection")
print(f"âœ… Used production-ready utilities with error handling")
print(f"âœ… Experienced progressive complexity (Beginner â†’ Enterprise)")
print(f"âœ… Gained tools for any scale of HK stock analysis")

print(f"\nğŸ“‹ **NEXT STEPS:**")
print(f"1. ğŸ”§ **Customize**: Modify configurations for your specific needs")
print(f"2. ğŸ“Š **Scale**: Apply to your target stock universe size")
print(f"3. ğŸ­ **Deploy**: Use enterprise features for production systems")
print(f"4. ğŸ“ˆ **Analyze**: Process your collected data with feature extraction")
print(f"5. ğŸ”„ **Iterate**: Refine and optimize based on your results")

print(f"\nğŸ’¡ **AVAILABLE UTILITIES:**")
print(f"   ğŸ“¦ BulkCollectionConfig - Centralized configuration")
print(f"   ğŸ”§ BulkCollector - Main collection engine")  
print(f"   ğŸ“Š ResultsManager - Data display and saving")
print(f"   ğŸ”° create_beginner_collector() - Safe defaults")
print(f"   âš¡ create_enterprise_collector() - High performance")

print(f"\n" + "="*70)
print(f"âœ… **SUCCESS!** You're now equipped for professional-grade HK stock data collection! ğŸ‡­ğŸ‡°ğŸ“ˆ")

# Save all configuration for reference
final_config = {
    'session_timestamp': datetime.now().isoformat(),
    'levels_completed': list(all_results.keys()),
    'total_stocks_processed': total_stocks if 'total_stocks' in locals() else 0,
    'configuration_used': config.__dict__
}

print(f"\nğŸ’¾ Session summary available in 'final_config' variable")

