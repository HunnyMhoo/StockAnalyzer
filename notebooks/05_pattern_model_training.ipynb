{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Pattern Model Training Notebook (Fixed)\n",
    "\n",
    "This notebook provides an interactive interface for training machine learning models to detect trading patterns.\n",
    "\n",
    "## Overview\n",
    "- Load and validate labeled feature data\n",
    "- Train multiple model types (XGBoost, Random Forest)\n",
    "- Evaluate model performance with comprehensive metrics\n",
    "- Compare models and select the best performer\n",
    "- Generate visualizations and reports\n",
    "\n",
    "## Prerequisites\n",
    "- Labeled features must be available in `../features/labeled_features.csv`\n",
    "- Ensure all required packages are installed (see requirements.txt)\n",
    "- On macOS, make sure you have libomp installed: `brew install libomp`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add src directory to path\n",
    "sys.path.append('../src')\n",
    "\n",
    "# Import our custom modules\n",
    "from pattern_model_trainer import PatternModelTrainer, TrainingConfig, load_trained_model\n",
    "from model_evaluator import ModelEvaluator\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"✅ All imports successful!\")\n",
    "print(f\"Working directory: {os.getcwd()}\")\n",
    "print(f\"Python path includes: {[p for p in sys.path if 'src' in p]}\")\n",
    "\n",
    "# Check XGBoost availability\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    print(\"✅ XGBoost is available\")\n",
    "except ImportError as e:\n",
    "    print(f\"❌ XGBoost import error: {e}\")\n",
    "    print(\"Install with: pip install xgboost\")\n",
    "    print(\"On macOS, also run: brew install libomp\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Step 1: Data Loading and Exploration\n",
    "\n",
    "Let's start by loading and examining our labeled feature data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Define data paths\n",
    "features_path = \"../features/labeled_features.csv\"\n",
    "models_dir = \"../models\"\n",
    "\n",
    "# Create models directory if it doesn't exist\n",
    "os.makedirs(models_dir, exist_ok=True)\n",
    "\n",
    "# Check if labeled features file exists\n",
    "if not os.path.exists(features_path):\n",
    "    print(f\"❌ Labeled features file not found at: {features_path}\")\n",
    "    print(\"Please ensure you have labeled feature data available.\")\n",
    "    print(\"You can generate sample data using generate_sample_training_data.py\")\n",
    "else:\n",
    "    print(f\"✅ Found labeled features file at: {features_path}\")\n",
    "    \n",
    "    # Load and examine the data\n",
    "    df = pd.read_csv(features_path)\n",
    "    print(f\"\\n📊 Dataset Shape: {df.shape}\")\n",
    "    print(f\"📊 Columns: {list(df.columns)}\")\n",
    "    \n",
    "    # Check for target column (should be 'label_type')\n",
    "    if 'label_type' in df.columns:\n",
    "        print(f\"📊 Label Distribution:\")\n",
    "        print(df['label_type'].value_counts())\n",
    "        positive_count = (df['label_type'] == 'positive').sum()\n",
    "        negative_count = (df['label_type'] == 'negative').sum()\n",
    "        print(f\"📊 Positive samples: {positive_count} ({positive_count/len(df)*100:.1f}%)\")\n",
    "        print(f\"📊 Negative samples: {negative_count} ({negative_count/len(df)*100:.1f}%)\")\n",
    "        \n",
    "        # Check if we have enough samples\n",
    "        if len(df) >= 30:\n",
    "            print(f\"✅ Sufficient samples for training ({len(df)} >= 30)\")\n",
    "        else:\n",
    "            print(f\"❌ Insufficient samples for training ({len(df)} < 30)\")\n",
    "            print(\"Run: python generate_sample_training_data.py to create more sample data\")\n",
    "    else:\n",
    "        print(\"❌ No 'label_type' target column found!\")\n",
    "        \n",
    "    print(f\"\\n📊 First few rows:\")\n",
    "    print(df.head())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Step 2: Training Configuration\n",
    "\n",
    "Define different training configurations for various model types and hyperparameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Define training configurations with correct parameter names\n",
    "configs = {\n",
    "    'xgboost_default': TrainingConfig(\n",
    "        model_type='xgboost',\n",
    "        test_size=0.2,\n",
    "        random_state=42,\n",
    "        cv_folds=5,\n",
    "        apply_smote=True,  # Correct parameter name\n",
    "        scale_features=True,\n",
    "        hyperparameters={  # Correct parameter name\n",
    "            'n_estimators': 100,\n",
    "            'max_depth': 6,\n",
    "            'learning_rate': 0.1,\n",
    "            'random_state': 42\n",
    "        }\n",
    "    ),\n",
    "    \n",
    "    'random_forest_default': TrainingConfig(\n",
    "        model_type='randomforest',  # Correct model type name\n",
    "        test_size=0.2,\n",
    "        random_state=42,\n",
    "        cv_folds=5,\n",
    "        apply_smote=True,\n",
    "        scale_features=False,  # RF doesn't need scaling\n",
    "        hyperparameters={\n",
    "            'n_estimators': 100,\n",
    "            'max_depth': 10,\n",
    "            'random_state': 42\n",
    "        }\n",
    "    )\n",
    "}\n",
    "\n",
    "print(\"📋 Available Configurations:\")\n",
    "for name, config in configs.items():\n",
    "    print(f\"  - {name}: {config.model_type}\")\n",
    "    \n",
    "# Select which configurations to train\n",
    "selected_configs = ['xgboost_default', 'random_forest_default']\n",
    "print(f\"\\n🎯 Selected configurations: {selected_configs}\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Step 3: Model Training\n",
    "\n",
    "Train the selected models and collect results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Dictionary to store training results\n",
    "training_results = {}\n",
    "\n",
    "# Train each selected configuration\n",
    "for config_name in selected_configs:\n",
    "    print(f\"\\n🚀 Training {config_name}...\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    try:\n",
    "        config = configs[config_name]\n",
    "        \n",
    "        # Initialize trainer with correct API\n",
    "        trainer = PatternModelTrainer(\n",
    "            features_file=features_path,\n",
    "            models_dir=models_dir,\n",
    "            config=config\n",
    "        )\n",
    "        \n",
    "        # Run training pipeline\n",
    "        results = trainer.train()\n",
    "        training_results[config_name] = results\n",
    "        \n",
    "        print(f\"✅ {config_name} training completed!\")\n",
    "        if results.cv_scores:\n",
    "            cv_mean = np.mean(list(results.cv_scores.values()))\n",
    "            cv_std = np.std(list(results.cv_scores.values()))\n",
    "            print(f\"   Cross-validation score: {cv_mean:.4f} ± {cv_std:.4f}\")\n",
    "        print(f\"   Test accuracy: {results.test_score['accuracy']:.4f}\")\n",
    "        print(f\"   Test F1 score: {results.test_score['f1_score']:.4f}\")  # Fixed key name\n",
    "        print(f\"   Test precision: {results.test_score['precision']:.4f}\")\n",
    "        print(f\"   Test recall: {results.test_score['recall']:.4f}\")\n",
    "        if results.model_path:\n",
    "            print(f\"   Model saved to: {results.model_path}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error training {config_name}: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        \n",
    "print(f\"\\n✅ Training completed for {len(training_results)} models\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Step 4: Model Evaluation and Comparison\n",
    "\n",
    "Compare the trained models and analyze their performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Create comparison summary\n",
    "if training_results:\n",
    "    print(\"🏆 MODEL COMPARISON SUMMARY\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Create comparison DataFrame\n",
    "    comparison_data = []\n",
    "    for config_name, results in training_results.items():\n",
    "        test_metrics = results.test_score\n",
    "        comparison_data.append({\n",
    "            'Model': config_name,\n",
    "            'Accuracy': test_metrics['accuracy'],\n",
    "            'Precision': test_metrics['precision'],\n",
    "            'Recall': test_metrics['recall'],\n",
    "            'F1 Score': test_metrics['f1_score'],\n",
    "            'ROC AUC': test_metrics.get('roc_auc', 0.0),\n",
    "            'Training Time (s)': results.training_time\n",
    "        })\n",
    "    \n",
    "    comparison_df = pd.DataFrame(comparison_data)\n",
    "    comparison_df = comparison_df.round(4)\n",
    "    \n",
    "    print(comparison_df.to_string(index=False))\n",
    "    \n",
    "    # Find best model by F1 score\n",
    "    best_model_idx = comparison_df['F1 Score'].idxmax()\n",
    "    best_model = comparison_df.loc[best_model_idx, 'Model']\n",
    "    best_f1 = comparison_df.loc[best_model_idx, 'F1 Score']\n",
    "    \n",
    "    print(f\"\\n🥇 Best Model: {best_model} (F1 Score: {best_f1:.4f})\")\n",
    "    \n",
    "    # Plot comparison if we have multiple models\n",
    "    if len(comparison_df) > 1:\n",
    "        metrics_to_plot = ['Accuracy', 'Precision', 'Recall', 'F1 Score', 'ROC AUC']\n",
    "        n_metrics = len(metrics_to_plot)\n",
    "        \n",
    "        fig, axes = plt.subplots(1, n_metrics, figsize=(15, 4))\n",
    "        \n",
    "        for i, metric in enumerate(metrics_to_plot):\n",
    "            ax = axes[i]\n",
    "            bars = ax.bar(comparison_df['Model'], comparison_df[metric])\n",
    "            ax.set_title(f'{metric} Comparison')\n",
    "            ax.set_ylim(0, 1)\n",
    "            ax.tick_params(axis='x', rotation=45)\n",
    "            \n",
    "            # Highlight best model for this metric\n",
    "            best_idx = comparison_df[metric].idxmax()\n",
    "            bars[best_idx].set_color('gold')\n",
    "            \n",
    "            # Add value labels on bars\n",
    "            for j, v in enumerate(comparison_df[metric]):\n",
    "                ax.text(j, v + 0.01, f'{v:.3f}', ha='center', va='bottom')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "else:\n",
    "    print(\"❌ No training results available for comparison\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Step 5: Feature Importance Analysis\n",
    "\n",
    "Analyze which features are most important for pattern detection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Analyze feature importance for each model\n",
    "for config_name, results in training_results.items():\n",
    "    if results.feature_importance is not None and not results.feature_importance.empty:\n",
    "        print(f\"\\n🔍 Feature Importance for {config_name}\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # Get top 10 most important features\n",
    "        importance_df = results.feature_importance.head(10)\n",
    "        print(importance_df.to_string(index=False))\n",
    "        \n",
    "        # Plot feature importance\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.barh(range(len(importance_df)), importance_df['importance'])\n",
    "        plt.yticks(range(len(importance_df)), importance_df['feature'])\n",
    "        plt.xlabel('Importance')\n",
    "        plt.title(f'Top 10 Feature Importance - {config_name}')\n",
    "        plt.gca().invert_yaxis()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(f\"❌ No feature importance data available for {config_name}\")\n",
    "\n",
    "print(\"\\n🎉 Pattern model training and analysis completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Quick test of the training results to verify they exist\n",
    "print(\"🔍 Current Training Results:\")\n",
    "print(f\"Number of trained models: {len(training_results)}\")\n",
    "\n",
    "for name, results in training_results.items():\n",
    "    print(f\"\\n📊 {name}:\")\n",
    "    print(f\"  - Test Accuracy: {results.test_score['accuracy']:.4f}\")\n",
    "    print(f\"  - Test F1 Score: {results.test_score['f1_score']:.4f}\")\n",
    "    print(f\"  - Test Precision: {results.test_score['precision']:.4f}\")\n",
    "    print(f\"  - Test Recall: {results.test_score['recall']:.4f}\")\n",
    "    print(f\"  - Training Time: {results.training_time:.2f}s\")\n",
    "    if results.model_path:\n",
    "        print(f\"  - Model Path: {results.model_path}\")\n",
    "\n",
    "print(\"\\n✅ Both models trained successfully! The KeyError was just a display issue, not a training failure.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent",
   "main_language": "python"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
