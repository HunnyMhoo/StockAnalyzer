{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Signal Outcome Tagging Notebook\n",
    "## User Story 2.2 - Tag Signal Outcome (Success or Failure)\n",
    "\n",
    "This notebook provides an interactive interface for manually tagging pattern match outcomes\n",
    "to track prediction accuracy and improve future model training through feedback loops.\n",
    "\n",
    "### Key Features:\n",
    "- Load and review pattern match files from `./signals/`\n",
    "- Tag individual matches with outcomes: success, failure, uncertain\n",
    "- Add detailed feedback notes for each prediction\n",
    "- Batch tagging capabilities for efficient processing\n",
    "- Feedback analysis and performance review by confidence bands\n",
    "- Safe file operations with automatic backups\n",
    "\n",
    "### Prerequisites:\n",
    "- Pattern match files generated from User Story 1.5 (Pattern Scanning)\n",
    "- Matches stored in `./signals/matches_YYYYMMDD.csv` format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add src directory to path for imports\n",
    "sys.path.append('../src')\n",
    "\n",
    "# Import signal outcome tagging functionality\n",
    "from signal_outcome_tagger import (\n",
    "    SignalOutcomeTagger,\n",
    "    SignalOutcomeError,\n",
    "    load_latest_matches,\n",
    "    quick_tag_outcome,\n",
    "    review_latest_feedback,\n",
    "    VALID_OUTCOMES\n",
    ")\n",
    "\n",
    "print(\"üì¶ Signal Outcome Tagging System Loaded\")\n",
    "print(f\"   Valid outcomes: {', '.join(VALID_OUTCOMES)}\")\n",
    "print(\"   Ready for pattern match feedback collection!\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## üîç Step 1: Discover Available Match Files\n",
    "\n",
    "First, let's check what pattern match files are available for tagging.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Initialize the tagger and discover available files\n",
    "signals_dir = '../signals'\n",
    "\n",
    "try:\n",
    "    tagger = SignalOutcomeTagger(signals_dir=signals_dir)\n",
    "    match_files = tagger.find_available_match_files()\n",
    "    \n",
    "    print(\"üìä Available Match Files for Tagging:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    if not match_files:\n",
    "        print(\"‚ùå No match files found!\")\n",
    "        print(\"   Please run pattern scanning (User Story 1.5) first to generate matches.\")\n",
    "    else:\n",
    "        for i, file_path in enumerate(match_files, 1):\n",
    "            filename = os.path.basename(file_path)\n",
    "            file_size = os.path.getsize(file_path)\n",
    "            \n",
    "            # Quick peek at file content\n",
    "            try:\n",
    "                temp_df = pd.read_csv(file_path)\n",
    "                match_count = len(temp_df)\n",
    "                confidence_range = f\"{temp_df['confidence_score'].min():.3f} - {temp_df['confidence_score'].max():.3f}\"\n",
    "                \n",
    "                print(f\"   {i}. {filename}\")\n",
    "                print(f\"      Matches: {match_count}, Confidence: {confidence_range}\")\n",
    "                print(f\"      Size: {file_size:,} bytes\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"   {i}. {filename} (Error reading: {e})\")\n",
    "        \n",
    "        print(f\"\\n‚úÖ Found {len(match_files)} match files ready for tagging\")\n",
    "        \n",
    "except SignalOutcomeError as e:\n",
    "    print(f\"‚ùå Error initializing tagger: {e}\")\n",
    "    print(\"   Please ensure the signals directory exists and contains match files.\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## üìÇ Step 2: Load and Review Match File\n",
    "\n",
    "Load the most recent match file and review its contents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Load the latest match file\n",
    "try:\n",
    "    file_path, matches_df = load_latest_matches(signals_dir)\n",
    "    \n",
    "    print(f\"üìÑ Loaded Match File: {os.path.basename(file_path)}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Display basic statistics\n",
    "    summary = tagger.get_match_summary(matches_df)\n",
    "    \n",
    "    print(f\"üìä Match File Summary:\")\n",
    "    print(f\"   Total matches: {summary['total_matches']}\")\n",
    "    print(f\"   Already tagged: {summary['tagged_count']}\")\n",
    "    print(f\"   Untagged: {summary['untagged_count']}\")\n",
    "    print(f\"   Confidence range: {summary['confidence_range'][0]:.3f} - {summary['confidence_range'][1]:.3f}\")\n",
    "    print(f\"   Date range: {summary['date_range'][0]} to {summary['date_range'][1]}\")\n",
    "    print(f\"   Tickers: {', '.join(summary['tickers'])}\")\n",
    "    \n",
    "    # Display the matches for review\n",
    "    print(f\"\\nüéØ Pattern Matches:\")\n",
    "    display_columns = ['ticker', 'window_start_date', 'window_end_date', 'confidence_score', 'rank']\n",
    "    \n",
    "    # Add outcome columns if they exist\n",
    "    if 'outcome' in matches_df.columns:\n",
    "        display_columns.extend(['outcome', 'feedback_notes'])\n",
    "    \n",
    "    display_df = matches_df[display_columns].copy()\n",
    "    \n",
    "    # Format confidence scores for better readability\n",
    "    display_df['confidence_score'] = display_df['confidence_score'].round(3)\n",
    "    \n",
    "    print(display_df.to_string(index=False, max_colwidth=20))\n",
    "    \n",
    "    print(f\"\\n‚úÖ Match file loaded successfully!\")\n",
    "    print(f\"   Ready for outcome tagging.\")\n",
    "    \n",
    "except SignalOutcomeError as e:\n",
    "    print(f\"‚ùå Error loading matches: {e}\")\n",
    "    matches_df = None\n",
    "    file_path = None"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## üè∑Ô∏è Step 3: Individual Match Tagging\n",
    "\n",
    "Tag individual matches with outcomes and feedback notes.\n",
    "\n",
    "### Usage Instructions:\n",
    "1. Identify the match you want to tag from the table above\n",
    "2. Use the ticker and window_start_date to uniquely identify it\n",
    "3. Choose outcome: 'success', 'failure', or 'uncertain'\n",
    "4. Add optional feedback notes explaining your reasoning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Example: Tag a single match outcome\n",
    "# Modify these parameters to tag your specific matches\n",
    "\n",
    "if matches_df is not None:\n",
    "    # Configuration for tagging - MODIFY THESE VALUES\n",
    "    ticker_to_tag = '0005.HK'  # Change to your target ticker\n",
    "    window_start_to_tag = '2024-08-19'  # Change to your target date\n",
    "    outcome_to_apply = 'success'  # Choose: 'success', 'failure', 'uncertain'\n",
    "    feedback_notes = 'Strong breakout after support test - volume confirmed the move'\n",
    "    \n",
    "    try:\n",
    "        # Apply the tag\n",
    "        updated_matches_df = tagger.tag_outcome(\n",
    "            matches_df,\n",
    "            ticker=ticker_to_tag,\n",
    "            window_start_date=window_start_to_tag,\n",
    "            outcome=outcome_to_apply,\n",
    "            feedback_notes=feedback_notes,\n",
    "            overwrite=False  # Set to True to overwrite existing tags\n",
    "        )\n",
    "        \n",
    "        print(f\"‚úÖ Successfully tagged {ticker_to_tag} ({window_start_to_tag}) as '{outcome_to_apply}'\")\n",
    "        \n",
    "        # Update our working DataFrame\n",
    "        matches_df = updated_matches_df\n",
    "        \n",
    "        # Show the updated row\n",
    "        tagged_row = matches_df[\n",
    "            (matches_df['ticker'] == ticker_to_tag) & \n",
    "            (matches_df['window_start_date'] == window_start_to_tag)\n",
    "        ]\n",
    "        \n",
    "        if not tagged_row.empty:\n",
    "            print(\"\\nüìã Tagged Match Details:\")\n",
    "            row = tagged_row.iloc[0]\n",
    "            print(f\"   Ticker: {row['ticker']}\")\n",
    "            print(f\"   Period: {row['window_start_date']} to {row['window_end_date']}\")\n",
    "            print(f\"   Confidence: {row['confidence_score']:.3f}\")\n",
    "            print(f\"   Outcome: {row['outcome']}\")\n",
    "            print(f\"   Notes: {row['feedback_notes']}\")\n",
    "            print(f\"   Tagged: {row['tagged_date']}\")\n",
    "        \n",
    "    except SignalOutcomeError as e:\n",
    "        print(f\"‚ùå Error tagging outcome: {e}\")\n",
    "        print(\"   Check ticker and date values, or use overwrite=True for existing tags\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No matches loaded. Please run the previous cell to load match data.\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## üíæ Step 4: Save Tagged Matches\n",
    "\n",
    "Save your tagged matches to a labeled file for future reference and analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Save the labeled matches\n",
    "if matches_df is not None and file_path is not None:\n",
    "    try:\n",
    "        # Save labeled matches with automatic naming\n",
    "        output_path = tagger.save_labeled_matches(matches_df, file_path)\n",
    "        \n",
    "        print(f\"‚úÖ Labeled matches saved successfully!\")\n",
    "        print(f\"   Output file: {os.path.basename(output_path)}\")\n",
    "        \n",
    "        # Verify saved content\n",
    "        saved_df = pd.read_csv(output_path)\n",
    "        tagged_count = (~saved_df['outcome'].isna()).sum()\n",
    "        \n",
    "        print(f\"\\nüìä Saved File Summary:\")\n",
    "        print(f\"   Total matches: {len(saved_df)}\")\n",
    "        print(f\"   Tagged matches: {tagged_count}\")\n",
    "        print(f\"   Untagged matches: {len(saved_df) - tagged_count}\")\n",
    "        \n",
    "        if tagged_count > 0:\n",
    "            outcome_counts = saved_df['outcome'].value_counts()\n",
    "            print(f\"\\nüéØ Outcome Distribution:\")\n",
    "            for outcome, count in outcome_counts.items():\n",
    "                percentage = (count / tagged_count * 100)\n",
    "                print(f\"   {outcome.title()}: {count} ({percentage:.1f}%)\")\n",
    "        \n",
    "    except SignalOutcomeError as e:\n",
    "        print(f\"‚ùå Error saving labeled matches: {e}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No matches to save. Please load and tag matches first.\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## üìà Step 5: Feedback Analysis and Review\n",
    "\n",
    "Analyze your tagging results to understand model performance across different confidence bands.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Analyze feedback results\n",
    "if matches_df is not None:\n",
    "    print(\"üìä Feedback Analysis\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Get detailed feedback analysis\n",
    "    feedback_results = tagger.review_feedback(matches_df)\n",
    "    \n",
    "    # Additional insights\n",
    "    if feedback_results['tagged_matches'] > 0:\n",
    "        print(\"\\nüîç Additional Insights:\")\n",
    "        \n",
    "        # Success rate by ticker (if multiple tickers)\n",
    "        tagged_matches = matches_df[matches_df['outcome'].notna()]\n",
    "        \n",
    "        if len(tagged_matches['ticker'].unique()) > 1:\n",
    "            print(\"\\nüìã Performance by Ticker:\")\n",
    "            ticker_performance = tagged_matches.groupby('ticker')['outcome'].value_counts().unstack(fill_value=0)\n",
    "            \n",
    "            for ticker in ticker_performance.index:\n",
    "                success = ticker_performance.loc[ticker].get('success', 0)\n",
    "                failure = ticker_performance.loc[ticker].get('failure', 0)\n",
    "                total_decisive = success + failure\n",
    "                \n",
    "                if total_decisive > 0:\n",
    "                    success_rate = success / total_decisive\n",
    "                    print(f\"   {ticker}: {success_rate:.1%} success rate ({success}S/{failure}F)\")\n",
    "        \n",
    "        # Recommendations based on results\n",
    "        print(\"\\nüí° Recommendations:\")\n",
    "        \n",
    "        overall_success_rate = feedback_results['outcome_summary']['counts'].get('success', 0) / max(1, feedback_results['tagged_matches'])\n",
    "        \n",
    "        if overall_success_rate > 0.7:\n",
    "            print(\"   ‚úÖ Model performing well - consider lowering confidence threshold\")\n",
    "        elif overall_success_rate < 0.5:\n",
    "            print(\"   ‚ö†Ô∏è  Model needs improvement - consider raising confidence threshold\")\n",
    "        else:\n",
    "            print(\"   üìä Model performance is moderate - continue collecting feedback\")\n",
    "        \n",
    "        if feedback_results['tagged_matches'] < 20:\n",
    "            print(\"   üìà Tag more matches for better statistical significance\")\n",
    "    \n",
    "    else:\n",
    "        print(\"\\nüí° No tagged matches found for analysis.\")\n",
    "        print(\"   Tag some matches using the cells above to see feedback analysis.\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No matches loaded for analysis.\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "ipynb,py:percent",
   "notebook_metadata_filter": "all,-language_info,-toc,-latex_envs"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
